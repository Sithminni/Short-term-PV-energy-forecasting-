# -*- coding: utf-8 -*-
"""Final Year Project  - Short Term PV energy Forecasting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SN-8mK9eHecsADaB1M1KmTJyY7HBtCzt

# INITIAL SETUP
"""

!pip install pvlib rdflib pyarrow fastparquet seaborn tqdm --quiet --no-deps

import os
from pathlib import Path
import re
import json
from tqdm import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import pvlib
from rdflib import Graph
from datetime import timedelta

sns.set(style='whitegrid', context='notebook', font_scale=1.0)
pd.options.display.max_columns = 200

"""Initial Configuration"""

# Cell 1 — Config (edit these paths to match your Drive mount)
BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")   # <<-- EDIT this
METEO_ROOT = BASE / "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Time series dataset/Meteorological dataset"                     # contains subfolders per weather station
PV_ROOT = BASE / "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Time series dataset/PV generation dataset"                         # contains with/without optimizer subfolders
METADATA_TTL = BASE / "metadata" / "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Metadata"    # optional
OUTPUT_DIR = BASE / "processed"
FIG_DIR = BASE / "reports" / "figures"

for p in [OUTPUT_DIR, FIG_DIR]:
    p.mkdir(parents=True, exist_ok=True)

print("BASE exists:", BASE.exists())
print("METEO root exists:", METEO_ROOT.exists())
print("PV root exists:", PV_ROOT.exists())
print("METADATA TTL exists:", METADATA_TTL.exists())

# Optional: show package versions for thesis documentation
import importlib, pkgutil
packages = ["pandas","numpy","scikit_learn","xgboost","tensorflow","pvlib","matplotlib","seaborn","plotly","joblib","streamlit","rdflib"]
from importlib import metadata
rows=[]
for p in packages:
    try:
        v = metadata.version(p)
    except Exception:
        v = "not installed / unknown"
    rows.append((p, v))
pd.DataFrame(rows, columns=["package","version"])

# Helper: list PV station files
def list_station_files(root):
    files = list(root.rglob("**/*"))
    # filter to typical datafiles
    files = [f for f in files if f.is_file() and f.suffix.lower() in [".csv",".parquet",".gz",".txt"]]
    return files

pv_files = list_station_files(PV_ROOT)
meteo_files = list_station_files(METEO_ROOT)
print(f"Found {len(pv_files)} PV files and {len(meteo_files)} meteo files")

# show some example paths (first 10)
pv_files[:10]

# read a representative PV file and meteo file (first readable CSV/parquet)
def try_read(path):
    try:
        if path.suffix.lower() in [".parquet"]:
            return pd.read_parquet(path)
        else:
            return pd.read_csv(path, nrows=2000)
    except Exception as e:
        try:
            return pd.read_csv(path, encoding='latin1', nrows=2000)
        except:
            print("Failed to read", path, type(e))
            return None

rep_pv = None
for p in pv_files:
    df = try_read(p)
    if df is not None:
        rep_pv = (p, df)
        break
rep_m = None
for p in meteo_files:
    df = try_read(p)
    if df is not None:
        rep_m = (p, df)
        break

print("Representative PV file:", rep_pv[0] if rep_pv else "None")
print("Representative METEO file:", rep_m[0] if rep_m else "None")

# function to guess time/columns and create a schema DF with example units mapping
def build_schema(df, suggested_units_map=None):
    cols = df.columns.tolist()
    schema = []
    # default units mapping (edit if dataset specifies differently)
    default_units = {
        'timestamp': 'ISO 8601',
        'time': 'ISO 8601',
        'datetime': 'ISO 8601',
        'panel_id': 'string',
        'site_id': 'string',
        'inverter_power': 'W',
        'power': 'W',
        'ac_power': 'W',
        'dc_power': 'W',
        'ghi': 'W/m^2',
        'dni': 'W/m^2',
        'dhi': 'W/m^2',
        'poa_irradiance': 'W/m^2',
        'temperature': '°C',
        'ambient_temp': '°C',
        'module_temp': '°C',
        'humidity': '%',
        'wind_speed': 'm/s'
    }
    for c in cols:
        key = c.lower().strip()
        unit = default_units.get(key, "")
        # if unit not found, try partial matching
        if unit == "":
            for k in default_units:
                if k in key:
                    unit = default_units[k]; break
        schema.append((c, unit))
    return pd.DataFrame(schema, columns=["column","suggested_unit"])

pv_schema_df = build_schema(rep_pv[1]) if rep_pv else pd.DataFrame()
meteo_schema_df = build_schema(rep_m[1]) if rep_m else pd.DataFrame()
print("PV schema (sample):")
display(pv_schema_df.head(30))
print("METEO schema (sample):")
display(meteo_schema_df.head(30))

# Save sample schema CSV (thesis appendix reference)
pv_schema_df.to_csv(BASE/"pv_sample_schema.csv", index=False)
meteo_schema_df.to_csv(BASE/"meteo_sample_schema.csv", index=False)

# This code assumes PV files are organized by station or subfolders like 'with_optimizer' and 'without_optimizer'
# Approach A: if folder names include "with_optimizer" or "without_optimizer", use that
from collections import Counter
station_info = []
for p in pv_files:
    # infer station id from filename or parent dir
    station_name = p.stem  # fallback
    parts = p.parts
    # try parent folder label
    parent = p.parent.name.lower()
    optim = None
    if "optimizer" in parent or "with" in parent:
        optim = "with_optimizer" if "with" in parent else "without_optimizer"
    station_info.append({"path":str(p), "parent":parent, "station":station_name, "optimizer":optim})
df_info = pd.DataFrame(station_info)
# quick counts
counts_by_parent = df_info.groupby("parent")['path'].nunique().sort_values(ascending=False)
print("Files by parent folder (top):")
display(counts_by_parent.head(20))

# If dataset contains folder 'with_optimizer' and 'without_optimizer' then:
with_count = df_info[df_info['parent'].str.contains('with', na=False)].station.nunique()
without_count = df_info[df_info['parent'].str.contains('without', na=False)].station.nunique()
print("Stations (approx) with optimizer:", with_count)
print("Stations (approx) without optimizer:", without_count)

# fallback: bar chart of number of files per parent folder
plt.figure(figsize=(10,5))
counts_by_parent.head(20).plot(kind='bar', color='tab:blue')
plt.title("Top 20 PV data parent folders (proxy for stations / categories)")
plt.ylabel("File count")
plt.tight_layout()
plt.show()

# For each PV station file, compute date range and percent completeness per day
# This can be slow if many large files; read only timestamps or downsample
def infer_time_col(df):
    # common names
    candidates = [c for c in df.columns if c.lower() in ['timestamp','time','datetime','date','ts']]
    if candidates:
        return candidates[0]
    # try heuristics
    for c in df.columns:
        if 'time' in c.lower() or 'date' in c.lower():
            return c
    return None

def station_coverage(file_list, max_rows=200000):
    rows=[]
    for p in file_list:
        df = try_read(p)
        if df is None:
            continue
        tcol = infer_time_col(df)
        if tcol is None:
            continue
        # ensure datetime
        try:
            ts = pd.to_datetime(df[tcol], errors='coerce')
        except:
            ts = pd.to_datetime(df[tcol].astype(str), errors='coerce')
        ts = ts.dropna()
        if ts.empty:
            continue
        sname = p.stem
        # group by date
        dcounts = ts.dt.floor('D').value_counts().sort_index()
        # expected count per day (if data originally 1-min) -> 1440
        expected = 1440  # adjust if your modeling uses resampling
        # percent completeness per day
        for d, c in dcounts.items():
            rows.append({"station":sname, "date":d, "count":int(c), "percent":(c/expected)*100})
    return pd.DataFrame(rows)

cov_df = station_coverage(pv_files[:100])  # try on subset first for speed, remove [:100] if OK
if cov_df.empty:
    print("No coverage info generated — check timestamp column names or file reading.")
else:
    # pivot to heatmap: stations x date (percent)
    pivot = cov_df.pivot_table(index='station', columns='date', values='percent', fill_value=0)
    # optionally reduce to a sample of stations for display if too large
    sample_pivot = pivot.iloc[:30, :]  # first 30 stations
    plt.figure(figsize=(16,8))
    sns.heatmap(sample_pivot, cmap="YlGnBu", cbar_kws={'label':'% completeness'})
    plt.title("Temporal coverage (% completeness) for sample stations (first 30)")
    plt.xlabel("Date")
    plt.ylabel("Station")
    plt.tight_layout()
    plt.show()

"""# DATA PREPROCESSING

Utilities - for timestamp detection
"""

# Cell 2 — Utility functions
def safe_read_csv(path, nrows=None):
    try:
        return pd.read_csv(path, nrows=nrows, low_memory=False)
    except Exception as e:
        print("Failed to read", path, "|", e)
        return None

def detect_timestamp_col(df):
    candidates = ['timestamp','date & time','date','time','datetime','datetime_utc','time_utc','Date & Time']
    for c in df.columns:
        if c.lower().strip() in [x.lower() for x in candidates]:
            return c
    # fallback: first col that pandas can parse into datetimes
    for c in df.columns:
        try:
            pd.to_datetime(df[c].iloc[:50])
            return c
        except Exception:
            continue
    return None

def downcast_numeric(df):
    for col in df.select_dtypes(include=['int64','float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float' if df[col].dtype=='float64' else 'integer')
    return df

"""# Ingest meteorological data (1-min)

Meteorological CSVs were read per station. Each station’s variables were pivoted into a wide table indexed by timestamp and saved in partitioned parquet files to manage memory
"""

# Cell 3 — Ingest meteorological station folders (1-min) for ALL stations or a chosen subset
# NOTE: reading full 3 years of 1-min data for many stations can be large. If memory issues occur, use Dask.

meteo_stations = sorted([p for p in METEO_ROOT.iterdir() if p.is_dir()])  # list all station folders
print("Detected meteorological station folders (count):", len(meteo_stations))

# We will process each station separately and save per-station parquet to avoid memory blowup.
meteo_out_dir = OUTPUT_DIR / "meteo_by_station"
meteo_out_dir.mkdir(parents=True, exist_ok=True)

for station_folder in tqdm(meteo_stations, desc="Stations"):
    station_name = station_folder.name
    out_path = meteo_out_dir / f"{station_name}.parquet"
    # if already processed, skip
    if out_path.exists():
        continue
    csvs = sorted([p for p in station_folder.glob("*.csv")])
    rows = []
    for f in csvs:
        df = safe_read_csv(f)
        if df is None:
            continue
        ts_col = detect_timestamp_col(df)
        if ts_col is None:
            print("No timestamp col in", f)
            continue
        df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')
        df = df.dropna(subset=[ts_col])
        # find measurement column(s) - take numeric columns
        val_cols = [c for c in df.columns if c != ts_col and pd.api.types.is_numeric_dtype(df[c])]
        if not val_cols:
            # fallback: take second column if present
            val_cols = [c for c in df.columns if c != ts_col]
        for c in val_cols:
            tmp = df[[ts_col, c]].rename(columns={ts_col: 'timestamp', c: 'value'}).copy()
            tmp['variable'] = Path(f).stem
            rows.append(tmp)
    if not rows:
        print("No valid files for", station_name)
        continue
    long = pd.concat(rows, ignore_index=True)
    long['timestamp'] = pd.to_datetime(long['timestamp'])
    wide = long.pivot_table(index='timestamp', columns='variable', values='value', aggfunc='mean').sort_index()
    # Save per-station parquet (this keeps memory usage bounded)
    wide.reset_index().to_parquet(out_path, index=False, compression='snappy')
    print("Saved", out_path)

"""# Ingest PV generation data (5-min)"""

# Cell 4 — Read PV generation CSVs from both subfolders and save partitioned by station
pv_subfolders = [p for p in PV_ROOT.iterdir() if p.is_dir()]
pv_out = OUTPUT_DIR / "pv_by_station"
pv_out.mkdir(parents=True, exist_ok=True)

all_station_files = []
for sub in pv_subfolders:
    for f in sorted(sub.glob("*.csv")):
        all_station_files.append((sub.name, f))

print("PV folders found:", [p.name for p in pv_subfolders])
print("PV files detected:", len(all_station_files))

for foldername, fpath in tqdm(all_station_files, desc="PV files"):
    station_id = Path(fpath).stem
    out_path = pv_out / f"{station_id}.parquet"
    if out_path.exists():
        continue
    df = safe_read_csv(fpath)
    if df is None:
        continue
    ts_col = detect_timestamp_col(df)
    if ts_col is None:
        print("No timestamp col in", fpath)
        continue
    df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')
    df = df.dropna(subset=[ts_col])
    # detect power column (heuristic)
    power_col = next((c for c in df.columns if re.search(r'power|pv|generation|kW|kw', c, flags=re.I)), None)
    if power_col is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c!=ts_col]
        if numeric_cols:
            power_col = numeric_cols[0]
        else:
            print("No power column in", fpath); continue
    df = df.rename(columns={ts_col: 'timestamp', power_col: 'pv_power'})
    df['station_id'] = station_id
    # mark optimizer flag from parent folder name
    df['has_optimizer'] = ('optimizer' in foldername.lower())
    df[['timestamp','station_id','pv_power','has_optimizer']].to_parquet(out_path, index=False, compression='snappy')
    print("Saved PV parquet for station:", station_id)

# @title
# DIAGNOSTIC: show PV folder structure and search for CSV/XLSX files
from pathlib import Path
import os
base_data_path = "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy"  # <- keep as you used
base = Path(base_data_path)

print("Base exists:", base.exists())
print("Base path (repr):", repr(str(base)))

pv_root = base / "pv_generation"
print("pv_root exists:", pv_root.exists())
if not pv_root.exists():
    # try to list the top-level of the base path to help you copy the correct one
    print("Top-level folders in base path:")
    for p in sorted(base.iterdir()):
        print("  ", "DIR" if p.is_dir() else "FILE", "-", p.name)
else:
    print("\nSubfolders under pv_generation:")
    for p in sorted(pv_root.iterdir()):
        print("  -", p.name, "(dir?)", p.is_dir())
    # Count .csv, .CSV, .xlsx files recursively and print first 30 examples
    csv_files = list(pv_root.rglob("*.csv"))
    csv_files_upper = list(pv_root.rglob("*.CSV"))
    xlsx_files = list(pv_root.rglob("*.xlsx"))
    other_files = list(pv_root.rglob("*.*"))
    print("\nCounts found recursively under pv_generation:")
    print("  .csv :", len(csv_files))
    print("  .CSV :", len(csv_files_upper))
    print("  .xlsx:", len(xlsx_files))
    print("  total files (with extension):", len(other_files))
    # Print first 30 file paths for inspection
    print("\nFirst 30 files found under pv_generation (if any):")
    for i, f in enumerate(other_files[:30]):
        print(f"  {i+1:02d}.", f.relative_to(base))
    # If no files, try listing pv_generation parent to help locate folder
    if len(other_files) == 0:
        print("\nNo files found inside pv_generation. Check these top-level folders instead:")
        for p in sorted(base.iterdir()):
            print("  ", p.name)

# @title
# Run this diagnostic & auto-detect cell in Colab (after Drive is mounted)
from pathlib import Path
import pandas as pd
import os

BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
ts_root = BASE / "Time series dataset"

print("BASE exists:", BASE.exists())
print("Time series dataset exists:", ts_root.exists())
if not ts_root.exists():
    raise SystemExit("Time series dataset folder not found at expected location. Please copy the correct path from the Files pane.")

# list top-level contents under Time series dataset
print("\nTop-level entries under 'Time series dataset':")
for p in sorted(ts_root.iterdir()):
    print(" ", "DIR" if p.is_dir() else "FILE", "-", p.name)

# recursively collect a sample of files and look for likely PV / meteo candidates
all_files = list(ts_root.rglob("*.*"))
print(f"\nTotal files found under Time series dataset (all types): {len(all_files)}")

# helper to test for keywords in filename and columns
def name_matches(fn, keywords):
    s = fn.name.lower()
    return any(k.lower() in s for k in keywords)

# find candidate folders that may contain meteorological files (look for common meteo names)
meteo_keywords = ['meteo', 'meteorol', 'ghi', 'radiation', 'temperature', 'humidity', 'wind', 'solar']
pv_keywords = ['pv', 'power', 'generation', 'kW', 'station', 'inverter']

candidate_meteo = set()
candidate_pv = set()

# scan directories and small sample of files
for f in all_files[:2000]:  # scan up to first 2000 files for speed
    parent = f.parent
    # check filename matches
    if name_matches(f, meteo_keywords):
        candidate_meteo.add(str(parent))
    if name_matches(f, pv_keywords):
        candidate_pv.add(str(parent))

# also inspect folder names directly
for folder in ts_root.rglob("*"):
    if folder.is_dir():
        if name_matches(folder, meteo_keywords):
            candidate_meteo.add(str(folder))
        if name_matches(folder, pv_keywords):
            candidate_pv.add(str(folder))

print("\nCandidate directories for meteorological data (by name/file patterns):")
for p in sorted(candidate_meteo):
    print("  -", Path(p).relative_to(BASE))

print("\nCandidate directories for PV generation data (by name/file patterns):")
for p in sorted(candidate_pv):
    print("  -", Path(p).relative_to(BASE))

# If nothing found by keywords, list subfolders so you can pick
if not candidate_meteo:
    print("\nNo meteorological candidates automatically detected. Please review subfolders below and pick the correct one:")
    for p in sorted(ts_root.iterdir()):
        if p.is_dir():
            print("  -", p.name)
if not candidate_pv:
    print("\nNo PV candidates automatically detected. Please review subfolders below and pick the correct one:")
    for p in sorted(ts_root.iterdir()):
        if p.is_dir():
            print("  -", p.name)

# show 3 example files from each candidate (if any) to help you inspect
def show_examples(paths, label):
    print(f"\nExamples for {label}:")
    n = 0
    for p in sorted(paths):
        p = Path(p)
        files = list(p.glob("*.*"))
        if files:
            print(" Folder:", p.relative_to(BASE))
            for f in files[:3]:
                print("   ", f.name)
                n += 1
        if n >= 12:
            break
    if n == 0:
        print("  (no example files found for this category)")

show_examples(candidate_meteo, "meteorological")
show_examples(candidate_pv, "pv generation")

# Summarize next steps for you
print("\nNEXT: If the candidate directories above look correct, copy the full path of the meteorological folder and the PV folder(s).")
print("You can paste them into variables METEO_ROOT and PV_ROOT (strings) and I will provide the ingestion code adapted exactly to those paths.")

import pandas as pd
import os
from pathlib import Path
from tqdm import tqdm

# --- 1. Define root folders ---
METEO_ROOT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Time series dataset/Meteorological dataset")
PV_ROOT_OPT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Time series dataset/PV generation dataset/PV stations with panel level optimizer/Inverter level dataset")
PV_ROOT_NO_OPT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/Time series dataset/PV generation dataset/PV stations without panel level optimizer/Site level dataset")

# Utilities for timestamp detection (re-defined for clarity within this cell)
def detect_timestamp_col(df):
    candidates = ['timestamp','date & time','date','time','datetime','datetime_utc','time_utc','Date & Time']
    for c in df.columns:
        if c.lower().strip() in [x.lower() for x in candidates]:
            return c
    # fallback: first col that pandas can parse into datetimes
    for c in df.columns:
        try:
            # check first 50 non-null values
            sample = df[c].dropna().head(50)
            if not sample.empty:
                pd.to_datetime(sample)
                return c
        except Exception:
            continue
    return None

# --- 2. Ingest Meteorological Data ---
print("\n--- Ingesting Meteorological Data ---")
all_weather_data = []

for var_folder in METEO_ROOT.iterdir():
    if var_folder.is_dir():
        variable_name = var_folder.name  # e.g., Temperature, Relative Humidity
        csv_files = list(var_folder.glob("*.csv"))
        if not csv_files:
             print(f"No CSV files found in {var_folder.name}")
             continue

        for f in tqdm(csv_files, desc=f"Processing {variable_name}"):
            df = pd.read_csv(f, low_memory=False)

            # Detect and standardize timestamp *before* processing values
            ts_col = detect_timestamp_col(df)
            if ts_col is None:
                print(f"Skipping file {f.name}: No timestamp column detected.")
                continue

            df = df.rename(columns={ts_col: 'timestamp'})
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            df = df.dropna(subset=['timestamp']) # Drop rows where timestamp parsing failed

            # Identify value column(s) - take numeric columns excluding timestamp
            value_cols = [c for c in df.columns if c != 'timestamp' and pd.api.types.is_numeric_dtype(df[c])]

            if not value_cols:
                # Fallback: if no numeric columns other than timestamp, try the next column
                other_cols = [c for c in df.columns if c != 'timestamp']
                if other_cols:
                    value_cols = [other_cols[0]]
                else:
                    print(f"Skipping file {f.name}: No value column detected.")
                    continue

            # Select only the timestamp and value columns, and add variable name
            # If multiple value columns are found, take the first one or refine logic
            if len(value_cols) > 1:
                 print(f"Warning: Multiple potential value columns found in {f.name}. Using the first one: {value_cols[0]}")

            df_processed = df[['timestamp', value_cols[0]]].copy()
            df_processed = df_processed.rename(columns={value_cols[0]: variable_name}) # Use folder name as column name
            all_weather_data.append(df_processed)

# Concatenate all processed dataframes
if all_weather_data:
    weather_df = pd.concat(all_weather_data, ignore_index=True)
    weather_df = weather_df.sort_values('timestamp')
    weather_df = weather_df.set_index('timestamp')

    print("\nWeather data summary:")
    print(weather_df.info())
    print(weather_df.head())

    # Optional: filter for a specific date range
    start_date = '2021-01-01'
    end_date = '2023-12-31'
    weather_df = weather_df.loc[start_date:end_date].copy()

    # Basic plot example for documentation
    import matplotlib.pyplot as plt
    # Plotting requires wide format, but the current structure is long.
    # Pivot the data for plotting if needed, but be mindful of memory with large datasets.
    # weather_pivot = weather_df.pivot_table(index='timestamp', columns='variable_name', values='value') # This was from previous attempt
    # A simpler plot per variable might be better for memory
    for var_name in weather_df.columns.unique():
        if var_name != 'variable_name': # Avoid plotting the variable name column itself
             plt.figure(figsize=(12, 4))
             weather_df[var_name].plot(title=f"{var_name} Data Overview")
             plt.ylabel(var_name)
             plt.show()


else:
    print("No meteorological data could be processed.")
    weather_df = pd.DataFrame() # Create empty dataframe to avoid errors later


# --- 3. Ingest PV Generation Data ---
print("\n--- Ingesting PV Generation Data ---")
all_pv_data = []

def ingest_pv_folder(pv_folder, has_optimizer):
    if not pv_folder.exists():
        print(f"PV folder not found: {pv_folder}")
        return

    for f in tqdm(list(pv_folder.glob("*.csv")), desc=f"Processing {'optimizer' if has_optimizer else 'no_optimizer'}"):
        df = pd.read_csv(f, low_memory=False)

        # Detect and standardize timestamp
        ts_col = detect_timestamp_col(df)
        if ts_col is None:
            print(f"Skipping file {f.name}: No timestamp column detected.")
            continue

        df = df.rename(columns={ts_col: 'timestamp'})
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df = df.dropna(subset=['timestamp']) # Drop rows where timestamp parsing failed

        # Standardize power column
        # Look for 'Power (kW)' first, then other keywords, then any numeric column
        power_col = None
        power_candidates = ['Power (kW)', 'power', 'pv', 'generation', 'kW', 'kw']
        for col in df.columns:
            if col in power_candidates:
                power_col = col
                break
            if any(keyword.lower() in col.lower() for keyword in power_candidates):
                 power_col = col
                 break

        if power_col is None:
            numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != 'timestamp']
            if numeric_cols:
                power_col = numeric_cols[0]
                print(f"Warning: Power column not explicitly named in {f.name}. Using first numeric column: {power_col}")
            else:
                print(f"Skipping file {f.name}: No power column found.")
                continue

        df = df.rename(columns={power_col: 'pv_power'})
        df['pv_power'] = pd.to_numeric(df['pv_power'], errors='coerce')


        # Extract station_id from filename
        station_id = f.stem
        df['station_id'] = station_id
        df['has_optimizer'] = has_optimizer

        # Select relevant columns before appending
        df_processed = df[['timestamp','station_id','pv_power','has_optimizer']].copy()
        all_pv_data.append(df_processed)

# Ingest both folders
ingest_pv_folder(PV_ROOT_OPT, True)
ingest_pv_folder(PV_ROOT_NO_OPT, False)

if all_pv_data:
    pv_df = pd.concat(all_pv_data, ignore_index=True)
    pv_df = pv_df.sort_values('timestamp')
    pv_df = pv_df.set_index('timestamp')

    print("\nPV generation data summary:")
    print(pv_df.info())
    print(pv_df.head())

    # Optional: filter by same date range as weather
    start_date = '2021-01-01'
    end_date = '2023-12-31'
    pv_df = pv_df.loc[start_date:end_date].copy()

    # Basic plot example for a few stations
    print("\nPlotting sample PV station data...")
    sample_stations = pv_df['station_id'].unique()[:3] # Plot data for up to 3 stations
    for station in sample_stations:
        plt.figure(figsize=(12, 4))
        pv_df[pv_df['station_id'] == station]['pv_power'].plot(title=f"PV Power Output for Station: {station}")
        plt.ylabel("Power (kW)")
        plt.show()

else:
    print("No PV generation data could be processed.")
    pv_df = pd.DataFrame() # Create empty dataframe to avoid errors later

"""# Resample Weather Data to 5-Minute Intervals

weather data is in 1 min interval while PV data is in 5 min interval therefore  resampling is necessary to match PV generation frequency.

 mean is chosen because it smooths short-term fluctuations while preserving overall trends.
"""

# --- Step 1: Resample Weather Data to 5-minute intervals ---
import pandas as pd
import matplotlib.pyplot as plt

# Pivot weather_df to wide format: each variable as a column # This is no longer needed as weather_df is already wide
# weather_df_pivot = weather_df.pivot_table(index='timestamp',
#                                           columns='variable_name',
#                                           values='value')

# Resample to 5-minute intervals using mean
# Ensure weather_df is indexed by timestamp for resampling
if not isinstance(weather_df.index, pd.DatetimeIndex):
    weather_df = weather_df.set_index('timestamp')

weather_df_resampled = weather_df.resample('5T').mean()

# Reset index for merging (if needed later, keep it as index for now)
# weather_df_resampled = weather_df_resampled.reset_index()

# Quick check
print(weather_df_resampled.head())
print(weather_df_resampled.info())

# Plot example: Temperature over time
# Ensure 'Temperature' column exists before plotting
if 'Temperature' in weather_df_resampled.columns:
    plt.figure(figsize=(12,5))
    plt.plot(weather_df_resampled.index, weather_df_resampled['Temperature'], color='orange') # Use index for plotting timestamp
    plt.title('Temperature Over Time (Resampled to 5-min)')
    plt.xlabel('Timestamp')
    plt.ylabel('Temperature (°C)')
    plt.show()
else:
    print("Temperature column not found in the resampled weather data.")

"""# Merge PV Generation and Weather Data

Left merge is used to preserve all PV generation data points.

Describe the resulting panel data structure with multi-index (timestamp, station_id).
"""

# --- Step 2: Merge PV Data with Resampled Weather Data ---

# Reset PV index to merge
pv_df_reset = pv_df.reset_index()

# Merge on timestamp
combined_df = pd.merge(pv_df_reset, weather_df_resampled, on='timestamp', how='left')

# Set MultiIndex for panel data structure
combined_df = combined_df.set_index(['timestamp', 'station_id']).sort_index()

# Quick check
print(combined_df.head())
print(combined_df.info())

# Check missing values
missing_values = combined_df.isnull().sum()
print("\nMissing values per column:\n", missing_values[missing_values>0])

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load merged dataset (if not already in memory)
combined_path = "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility/Research Data/Dataset - original copy/processed/combined_panel.parquet"
combined_df = pd.read_parquet(combined_path)

# --- 1. Basic information ---
print("Dataset shape:", combined_df.shape)
print("\nData types:")
print(combined_df.dtypes)
print("\nSample records:")
display(combined_df.head())

# --- 2. Summary statistics ---
print("\nStatistical Summary:")
display(combined_df.describe().T)

# --- 3. Missing value summary ---
missing_summary = combined_df.isnull().mean().sort_values(ascending=False) * 100
missing_summary = missing_summary.round(2)
print("\nMissing Data Percentage per Column (%):")
print(missing_summary[missing_summary > 0])

# --- 4. Count of unique stations and time coverage ---
stations = combined_df.index.get_level_values("station_id").unique()
print(f"\nNumber of unique PV stations: {len(stations)}")

# Date range (assuming timestamp is in MultiIndex)
timestamps = combined_df.index.get_level_values("timestamp")
print(f"Data coverage: {timestamps.min()} → {timestamps.max()}")
print(f"Total duration: {(timestamps.max() - timestamps.min()).days} days")

# --- 5. Correlation heatmap for numeric features ---
numeric_cols = combined_df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns
plt.figure(figsize=(10, 6))
sns.heatmap(combined_df[numeric_cols].corr(), cmap="Blues", annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

# --- 6. Distribution plot of PV output ---
plt.figure(figsize=(8, 4))
sns.histplot(combined_df['pv_power'].dropna(), kde=True, color='orange')
plt.title("Distribution of PV Power Output (kW)")
plt.xlabel("PV Power (kW)")
plt.ylabel("Frequency")
plt.show()

import pandas as pd
from pathlib import Path

# Define your processed folder (create if missing)
OUTPUT_DIR = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility/Research Data/Dataset - original copy/processed")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# If you still have weather_df_resampled and pv_df, merge them again:
pv_df_reset = pv_df.reset_index()
combined_df = pd.merge(pv_df_reset, weather_df_resampled, on="timestamp", how="left")

# Set MultiIndex for panel structure
combined_df = combined_df.set_index(["station_id", "timestamp"]).sort_index()

# Save for later reuse
combined_df.to_parquet(OUTPUT_DIR / "combined_panel.parquet")

print("✅ Combined dataset created and saved:", combined_df.shape)

combined_df = pd.read_parquet(OUTPUT_DIR / "combined_panel.parquet")

"""# FEATURE ENGINEERING

# basic temporal features, lag features, and rolling statistics
"""

# --- CREATE combined_panel.parquet (run this cell in Colab) ---
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import re

BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
TS_ROOT = BASE / "Time series dataset"
METEO_ROOT = TS_ROOT / "Meteorological dataset"
PV_ROOT_BASE = TS_ROOT / "PV generation dataset"
OUT_DIR = BASE / "processed"
OUT_DIR.mkdir(parents=True, exist_ok=True)
MERGED_OUT = OUT_DIR / "combined_panel.parquet"

print("BASE exists:", BASE.exists())
print("METEO_ROOT:", METEO_ROOT.exists(), METEO_ROOT)
print("PV_ROOT_BASE:", PV_ROOT_BASE.exists(), PV_ROOT_BASE)
print("OUT_DIR:", OUT_DIR)

# 1) Read & assemble meteorological data (pivot & resample to 5-min)
print("\nReading meteorological data...")
meteo_var_folders = [p for p in METEO_ROOT.iterdir() if p.is_dir()]
meteo_records = []
for var_folder in meteo_var_folders:
    varname = var_folder.name
    csvs = sorted(var_folder.glob("*.csv"))
    for f in tqdm(csvs, desc=f"Reading {varname}"):
        try:
            df = pd.read_csv(f, low_memory=False)
        except Exception as e:
            print("  Failed to read", f.name, "|", e); continue
        # detect timestamp column
        ts_col = next((c for c in df.columns if c.lower().strip() in ('date & time','date','timestamp','datetime','time')), None)
        if ts_col is None:
            for c in df.columns:
                try:
                    pd.to_datetime(df[c].iloc[:20])
                    ts_col = c; break
                except Exception:
                    pass
        if ts_col is None:
            print("  Skip (no timestamp):", f.name); continue
        df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')
        df = df.dropna(subset=[ts_col])
        # pick numeric value column
        val_col = next((c for c in df.columns if c!=ts_col and pd.api.types.is_numeric_dtype(df[c])), None)
        if val_col is None:
            other_cols = [c for c in df.columns if c!=ts_col]
            if not other_cols:
                print("  Skip (no value col):", f.name); continue
            val_col = other_cols[0]
        tmp = df[[ts_col, val_col]].rename(columns={ts_col:'timestamp', val_col:'value'}).copy()
        tmp['variable'] = varname
        meteo_records.append(tmp)

if not meteo_records:
    raise SystemExit("No meteorological CSVs read. Check the METEO_ROOT path and CSV availability.")
meteo_long = pd.concat(meteo_records, ignore_index=True)
meteo_long['timestamp'] = pd.to_datetime(meteo_long['timestamp'])
meteo_wide = meteo_long.pivot_table(index='timestamp', columns='variable', values='value', aggfunc='mean')
meteo_5min = meteo_wide.resample('5T').mean().reset_index()
print("Meteorological resampled to 5-min. Shape:", meteo_5min.shape)
print("Weather columns (sample):", list(meteo_5min.columns[:10]))

# 2) Read PV CSVs (both optimizer & no-optimizer) recursively
print("\nReading PV files...")
pv_folders = [
    PV_ROOT_BASE / "PV stations with panel level optimizer" / "Inverter level dataset",
    PV_ROOT_BASE / "PV stations without panel level optimizer" / "Site level dataset"
]
# include any other subfolders in PV_ROOT_BASE
for p in PV_ROOT_BASE.iterdir():
    if p.is_dir() and p not in pv_folders:
        pv_folders.append(p)

pv_records = []
total_files = 0
for folder in pv_folders:
    if not folder.exists():
        print("  PV folder not found:", folder); continue
    csvs = sorted(folder.rglob("*.csv"))
    print("  Scanning", folder, "->", len(csvs), "CSV files")
    for f in csvs:
        total_files += 1
        try:
            df = pd.read_csv(f, low_memory=False)
        except Exception as e:
            print("   Failed read", f.name, "|", e); continue
        ts_col = next((c for c in df.columns if c.lower().strip() in ('date & time','date','timestamp','datetime','time')), None)
        if ts_col is None:
            for c in df.columns:
                try:
                    pd.to_datetime(df[c].iloc[:20])
                    ts_col = c; break
                except Exception:
                    pass
        if ts_col is None:
            print("   Skip (no timestamp):", f.name); continue
        df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')
        df = df.dropna(subset=[ts_col])
        power_col = next((c for c in df.columns if re.search(r'power|pv|generation|kW|kw', c, flags=re.I)), None)
        if power_col is None:
            numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c!=ts_col]
            power_col = numeric[0] if numeric else None
        if power_col is None:
            print("   Skip (no power col):", f.name); continue
        tmp = df[[ts_col, power_col]].rename(columns={ts_col:'timestamp', power_col:'power_output_kw'}).copy()
        station_id = f.stem
        tmp['station_id'] = station_id
        tmp['has_optimizer'] = ('optimizer' in str(folder).lower())
        pv_records.append(tmp)

print("PV files scanned:", total_files, "PV records frames:", len(pv_records))
if not pv_records:
    raise SystemExit("No PV CSVs read. Check PV folders and CSV files.")

pv_long = pd.concat(pv_records, ignore_index=True)
pv_long['timestamp'] = pd.to_datetime(pv_long['timestamp'])
pv_long['timestamp'] = pv_long['timestamp'].dt.floor('5min')  # floor to 5-min
print("PV long shape:", pv_long.shape, "unique stations:", pv_long['station_id'].nunique())

# 3) Merge PV + weather (left join on timestamp)
print("\nMerging PV with 5-min weather (left join)...")
merged = pd.merge(pv_long, meteo_5min, on='timestamp', how='left')
merged = merged.sort_values(['station_id','timestamp']).reset_index(drop=True)
# Save as parquet
merged.to_parquet(MERGED_OUT, index=False, compression='snappy')
print("Saved merged panel to:", MERGED_OUT)
print("Merged shape:", merged.shape)
print("Columns sample:", merged.columns.tolist()[:30])

from pathlib import Path
OUTPUT_DIR = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
print("processed folder exists?", OUTPUT_DIR.exists())

# common names we expect
candidates = [
    OUTPUT_DIR / "combined_panel.parquet",
    OUTPUT_DIR / "panel_with_metadata.parquet",
    OUTPUT_DIR / "panel_with_clearness.parquet",
    OUTPUT_DIR / "panel_features_imputed.parquet",
    OUTPUT_DIR / "panel.parquet"
]

existing = [p for p in candidates if p.exists()]
if existing:
    print("Found merged file(s):")
    for p in existing:
        print(" -", p)
else:
    # list any parquet files in folder
    parquets = list(OUTPUT_DIR.glob("*.parquet"))
    if parquets:
        print("No standard-named merged file found. Using first parquet in processed/:")
        print(" -", parquets[0])
    else:
        print("No parquet files found in processed/. You must run the merge cell to create combined_panel.parquet first.")

from pathlib import Path
import pandas as pd
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
P = OUT / "combined_panel.parquet"
print("File exists:", P.exists())
df = pd.read_parquet(P, columns=['timestamp','station_id']).head(10)
print(df)

"""saved the merged panel to processed/combined_panel.parquet — contains PV + 5-min resampled meteorology

# Creating temporal **features**

engineered temporal features (hour, minute, day-of-week, month, day-of-year), weekend flag, and cyclical encodings (sine/cosine of hour and day-of-year) to capture diurnal and seasonal patterns
"""

import pandas as pd
from pathlib import Path
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
panel = pd.read_parquet(OUT / "combined_panel.parquet")
panel['timestamp'] = pd.to_datetime(panel['timestamp'])

# temporal features
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)
panel['hour'] = panel['timestamp'].dt.hour
panel['minute'] = panel['timestamp'].dt.minute
panel['dow'] = panel['timestamp'].dt.dayofweek
panel['month'] = panel['timestamp'].dt.month
panel['day_of_year'] = panel['timestamp'].dt.dayofyear
panel['is_weekend'] = panel['dow'].isin([5,6]).astype(int)
panel['hour_sin'] = np.sin(2*np.pi*panel['hour']/24)
panel['hour_cos'] = np.cos(2*np.pi*panel['hour']/24)
panel['doy_sin'] = np.sin(2*np.pi*panel['day_of_year']/365.25)
panel['doy_cos'] = np.cos(2*np.pi*panel['day_of_year']/365.25)

panel.to_parquet(OUT / "panel_temp_features.parquet", index=False, compression='snappy')
print("Saved: panel_temp_features.parquet, shape:", panel.shape)

import pandas as pd
import numpy as np # Import numpy for temporal features
import matplotlib.pyplot as plt
from pathlib import Path

OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility/Research Data/Dataset - original copy/processed")
# Load the combined panel file which exists
panel = pd.read_parquet(OUT / "combined_panel.parquet")

# Reset index to make 'timestamp' a column again if it was part of a MultiIndex
if isinstance(panel.index, pd.MultiIndex) and 'timestamp' in panel.index.names:
    panel = panel.reset_index()

# Ensure timestamp is datetime and sort
panel['timestamp'] = pd.to_datetime(panel['timestamp'])
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

# Add temporal features (copied from the previous cell)
panel['hour'] = panel['timestamp'].dt.hour
panel['minute'] = panel['timestamp'].dt.minute
panel['dow'] = panel['timestamp'].dt.dayofweek
panel['month'] = panel['timestamp'].dt.month
panel['day_of_year'] = panel['timestamp'].dt.dayofyear
panel['is_weekend'] = panel['dow'].isin([5,6]).astype(int)
panel['hour_sin'] = np.sin(2*np.pi*panel['hour']/24)
panel['hour_cos'] = np.cos(2*np.pi*panel['hour']/24)
panel['doy_sin'] = np.sin(2*np.pi*panel['day_of_year']/365.25)
panel['doy_cos'] = np.cos(2*np.pi*panel['day_of_year']/365.25)


# Example: pick one representative station
# Need to handle case where 'power_output_kw' is not the column name
power_col_name = 'power_output_kw'
if power_col_name not in panel.columns:
    # Attempt to find a suitable column based on previous attempts or common names
    potential_power_cols = [c for c in panel.columns if 'power' in c.lower() or 'generation' in c.lower() or 'pv' in c.lower()]
    if potential_power_cols:
        power_col_name = potential_power_cols[0]
        print(f"Warning: '{power_col_name}' not found. Using '{power_col_name}' as power column.")
    else:
        print("Error: Could not find a suitable power column name.")
        power_col_name = None # Set to None to avoid errors later

if power_col_name:
    # Ensure the power column is numeric
    panel[power_col_name] = pd.to_numeric(panel[power_col_name], errors='coerce')
    example = panel[panel['station_id'] == panel['station_id'].unique()[0]].copy() # Use .copy() to avoid SettingWithCopyWarning


    # Aggregate mean power by hour
    mean_profile = example.groupby('hour')[power_col_name].mean()

    # Plot
    plt.figure(figsize=(8,4))
    plt.plot(mean_profile.index, mean_profile.values, marker='o', color='#1f77b4')
    plt.title(f"Mean Diurnal PV Generation Profile – Example Station ({example['station_id'].iloc[0]})")
    plt.xlabel("Hour of Day")
    plt.ylabel(f"Mean PV Output ({power_col_name})")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.show()
else:
    print("Plotting skipped due to missing power column.")

import numpy as np
import matplotlib.pyplot as plt

hours = np.arange(0, 24)
hour_sin = np.sin(2 * np.pi * hours / 24)
hour_cos = np.cos(2 * np.pi * hours / 24)

plt.figure(figsize=(5,5))
plt.plot(hour_cos, hour_sin, 'o-', color='#2ca02c')
for i, h in enumerate(hours):
    if h % 6 == 0:  # label every 6 hours
        plt.text(hour_cos[i], hour_sin[i], str(h), fontsize=9, ha='right')
plt.title("Cyclical Encoding of Hour (sin–cos mapping)")
plt.xlabel("hour_cos")
plt.ylabel("hour_sin")
plt.axis('equal')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""# Adding Short Lag features

Short lag features (5–60 minutes) were added to capture recent autocorrelation in PV output
"""

import pandas as pd
from pathlib import Path
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
panel = pd.read_parquet(OUT / "panel_temp_features.parquet")
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

short_lags = [1,2,3,6,12]  # steps (1 step = 5 min)
for lag in short_lags:
    panel[f'pv_lag_{lag}'] = panel.groupby('station_id')['power_output_kw'].shift(lag)

panel.to_parquet(OUT / "panel_with_lags_short.parquet", index=False, compression='snappy')
print("Saved: panel_with_lags_short.parquet, added lags:", short_lags)

"""# **Rolling Statistics**

computed rolling mean and standard deviation over short windows to capture local trends and volatility
"""

import pandas as pd
from pathlib import Path
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
panel = pd.read_parquet(OUT / "panel_with_lags_short.parquet")
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

windows = [3,6,12]  # 15min, 30min, 60min windows (3=15min at 5-min steps)
for w in windows:
    panel[f'pv_roll_mean_{w}'] = panel.groupby('station_id')['power_output_kw'].transform(lambda s: s.rolling(window=w, min_periods=1).mean().shift(1))
    panel[f'pv_roll_std_{w}']  = panel.groupby('station_id')['power_output_kw'].transform(lambda s: s.rolling(window=w, min_periods=1).std().shift(1))

panel.to_parquet(OUT / "panel_with_rolls.parquet", index=False, compression='snappy')
print("Saved: panel_with_rolls.parquet — rolling windows:", windows)

"""# **Multi-horizon targets (1h, 4h, 24h)**

Targets were created for multiple horizons: 1-hour, 4-hour, and 24-hour ahead (12, 48, 288 5-minute steps respectively)
"""

import pandas as pd
from pathlib import Path
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
panel = pd.read_parquet(OUT / "panel_with_rolls.parquet")
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

horizons = {'h_1h':12, 'h_4h':48, 'h_24h':288}
for name, steps in horizons.items():
    panel[f'target_pv_{name}'] = panel.groupby('station_id')['power_output_kw'].shift(-steps)

panel.to_parquet(OUT / "panel_features_partial.parquet", index=False, compression='snappy')
print("Saved: panel_features_partial.parquet — targets added.")

"""missingness & station sample checks"""

import pandas as pd
from pathlib import Path
OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
panel = pd.read_parquet(OUT / "panel_features_partial.parquet")
print("Shape:", panel.shape)
print("Missing values (top 20):")
print(panel.isna().sum().sort_values(ascending=False).head(20))

# show sample rows for one station
st = panel['station_id'].unique()[0]
print("Sample station:", st)
display(panel[panel['station_id']==st].head(10))

"""session crashed afer using all available RAM during previous step

"""

import gc, psutil, os
gc.collect()
print("Process memory (MB):", psutil.Process(os.getpid()).memory_info().rss / 1024**2)

import pandas as pd
from pathlib import Path

BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
METEO_ROOT = BASE / "Time series dataset" / "Meteorological dataset"
OUT = BASE / "processed"
OUT.mkdir(parents=True, exist_ok=True)
METEO_5MIN_PATH = OUT / "meteo_5min.parquet"

# If you already have meteo_5min in memory, save it. Otherwise build from CSVs:
if not METEO_5MIN_PATH.exists():
    records = []
    for var_folder in sorted(METEO_ROOT.iterdir()):
        if not var_folder.is_dir():
            continue
        varname = var_folder.name
        for f in sorted(var_folder.glob("*.csv")):
            df = pd.read_csv(f, low_memory=False)
            # detect timestamp column
            ts_col = next((c for c in df.columns if c.lower().strip() in ('date & time','date','timestamp','datetime','time')), None)
            if ts_col is None:
                for c in df.columns:
                    try:
                        pd.to_datetime(df[c].iloc[:20])
                        ts_col = c; break
                    except Exception:
                        pass
            if ts_col is None:
                continue
            df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')
            df = df.dropna(subset=[ts_col])
            value_col = next((c for c in df.columns if c!=ts_col and pd.api.types.is_numeric_dtype(df[c])), None)
            if value_col is None:
                continue
            tmp = df[[ts_col, value_col]].rename(columns={ts_col:'timestamp', value_col:'value'}).copy()
            tmp['variable'] = varname
            records.append(tmp)
    meteo_long = pd.concat(records, ignore_index=True)
    meteo_long['timestamp'] = pd.to_datetime(meteo_long['timestamp'])
    meteo_wide = meteo_long.pivot_table(index='timestamp', columns='variable', values='value', aggfunc='mean')
    meteo_5min = meteo_wide.resample('5min').mean().reset_index()
    meteo_5min.to_parquet(METEO_5MIN_PATH, index=False, compression='snappy')
    print("Saved meteo_5min:", METEO_5MIN_PATH)
else:
    meteo_5min = pd.read_parquet(METEO_5MIN_PATH)
    print("Loaded meteo_5min from:", METEO_5MIN_PATH)

print("meteo_5min shape:", meteo_5min.shape)

"""# Creating a Sub dataset

Choosing representative stations (N = 10 by default)

inspects station stats (count, 99th percentile power, missing fraction, optimizer status) and picks a diverse set: some high-capacity, some mid, some low, and both optimizer types
"""

# CELL A — Pick representative stations
import pandas as pd
from pathlib import Path

OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
MERGED = OUT / "combined_panel.parquet"   # created earlier

print("Loading merged panel (head only to inspect)...")
df = pd.read_parquet(MERGED, columns=['station_id','power_output_kw','has_optimizer'])
print("Rows loaded for stats:", len(df))

# compute station-level stats
stats = df.groupby('station_id').agg(
    n_obs = ('power_output_kw','count'),
    p99 = ('power_output_kw', lambda s: float(s.quantile(0.99)) if s.dropna().size>0 else float('nan')),
    missing_frac = ('power_output_kw', lambda s: float(s.isna().mean()))
).reset_index()

# attach optimizer info (if present)
opt = df[['station_id','has_optimizer']].drop_duplicates().set_index('station_id')
stats['has_optimizer'] = stats['station_id'].map(opt['has_optimizer'])

# quick sorting and display
stats_sorted = stats.sort_values(['p99','n_obs'], ascending=[False, False]).reset_index(drop=True)
display(stats_sorted.head(12))

# selection logic: top / mid / bottom mixes
N = 10  # change this number to pick more stations
top = stats_sorted.head(N//3)['station_id'].tolist()
mid_start = len(stats_sorted)//3
mid = stats_sorted.iloc[mid_start:mid_start + (N//3)]['station_id'].tolist()
bot = stats_sorted.tail(max(0, N - len(top) - len(mid)))['station_id'].tolist()
selected = top + mid + bot

# ensure we have both optimizer types represented — add one if missing
if True not in stats_sorted[stats_sorted['station_id'].isin(selected)]['has_optimizer'].values and True in stats_sorted['has_optimizer'].values:
    add = stats_sorted[stats_sorted['has_optimizer']==True].iloc[0]['station_id']
    selected.append(add)
if False not in stats_sorted[stats_sorted['station_id'].isin(selected)]['has_optimizer'].values and False in stats_sorted['has_optimizer'].values:
    add = stats_sorted[stats_sorted['has_optimizer']==False].iloc[0]['station_id']
    selected.append(add)

selected = list(dict.fromkeys(selected))  # dedupe preserve order
print(f"\nSelected {len(selected)} stations for prototype:")
print(selected)

# save list for reuse
sel_file = OUT / "selected_stations.txt"
sel_file.write_text("\n".join(selected))
print("Saved selected stations to:", sel_file)

"""Create and save subset parquet (prototyping dataset)

"""

# CELL B — Build subset parquet from selected stations
import pandas as pd
from pathlib import Path

OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
MERGED = OUT / "combined_panel.parquet"
SEL = OUT / "selected_stations.txt"
SUBSET_OUT = OUT / "combined_panel_subset.parquet"

selected = [s.strip() for s in SEL.read_text().splitlines() if s.strip()]
print("Selected stations:", selected)

# read merged file in chunks if memory is constrained; here we can filter with pandas read_parquet
# parquet read supports row filtering poorly, so read whole and filter (should be OK for subset creation)
merged = pd.read_parquet(MERGED)
subset = merged[merged['station_id'].isin(selected)].copy()
print("Subset shape:", subset.shape)
subset.to_parquet(SUBSET_OUT, index=False, compression='snappy')
print("Saved subset to:", SUBSET_OUT)

"""Feature-engineer the subset"""

# CELL C — Feature-engineer subset (temporal, lags, small rolling, targets)
import pandas as pd
import numpy as np
from pathlib import Path

OUT = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed")
SUBSET_IN = OUT / "combined_panel_subset.parquet"
FEATURES_OUT = OUT / "panel_features_subset.parquet"

print("Loading subset:", SUBSET_IN)
panel = pd.read_parquet(SUBSET_IN)
panel['timestamp'] = pd.to_datetime(panel['timestamp'])
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

# temporal features
panel['hour'] = panel['timestamp'].dt.hour
panel['minute'] = panel['timestamp'].dt.minute
panel['dow'] = panel['timestamp'].dt.dayofweek
panel['month'] = panel['timestamp'].dt.month
panel['day_of_year'] = panel['timestamp'].dt.dayofyear
panel['is_weekend'] = panel['dow'].isin([5,6]).astype(int)
panel['hour_sin'] = np.sin(2*np.pi*panel['hour']/24)
panel['hour_cos'] = np.cos(2*np.pi*panel['hour']/24)
panel['doy_sin'] = np.sin(2*np.pi*panel['day_of_year']/365.25)
panel['doy_cos'] = np.cos(2*np.pi*panel['day_of_year']/365.25)

# short lags (prototype)
short_lags = [1,2,3,6,12]  # 5min..1h
for lag in short_lags:
    panel[f'pv_lag_{lag}'] = panel.groupby('station_id')['power_output_kw'].shift(lag)

# rolling stats (small windows)
windows = [3,6,12]  # 15, 30, 60 min
for w in windows:
    panel[f'pv_roll_mean_{w}'] = panel.groupby('station_id')['power_output_kw'].transform(lambda s: s.rolling(window=w, min_periods=1).mean().shift(1))
    panel[f'pv_roll_std_{w}']  = panel.groupby('station_id')['power_output_kw'].transform(lambda s: s.rolling(window=w, min_periods=1).std().shift(1))

# create targets for horizons
horizons = {'h_1h':12, 'h_4h':48, 'h_24h':288}
for name, steps in horizons.items():
    panel[f'target_pv_{name}'] = panel.groupby('station_id')['power_output_kw'].shift(-steps)

# estimate capacity (99th percentile) and normalized pv
cap_est = panel.groupby('station_id')['power_output_kw'].quantile(0.99).reset_index().rename(columns={'power_output_kw':'capacity_kw'})
panel = panel.merge(cap_est, on='station_id', how='left')
panel['pv_norm'] = panel['power_output_kw'] / (panel['capacity_kw'] + 1e-9)

# save
panel.to_parquet(FEATURES_OUT, index=False, compression='snappy')
print("Saved feature-engineered subset to:", FEATURES_OUT)
print("Subset features shape:", panel.shape)

"""# **ML MODEL TRAINING & EVALUATION**

Loading subset dataset
"""

import pandas as pd
from pathlib import Path

# Path to your subset features (adjust if needed)
SUBSET_FILE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed/panel_features_subset.parquet")

# Load subset
panel = pd.read_parquet(SUBSET_FILE)

# Quick check
print("Loaded panel shape:", panel.shape)
print("Columns:", panel.columns.tolist())
panel.head()

"""Selecting features and target"""

# Target column
TARGET = 'power_output_kw'

# Select features (exclude target, timestamp, station_id, etc.)
EXCLUDE = ['timestamp', 'station_id', 'power_output_kw', 'has_optimizer', 'clearness_index']
features = [c for c in panel.columns if c not in EXCLUDE]

X = panel[features]
y = panel[TARGET]

print("Number of features:", len(features))
print("Sample features:", features[:33])

"""Splitting the dataset into training and testing"""

from sklearn.model_selection import train_test_split

# 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# Inspect NaNs in targets (helpful debug)
print("\nMissing target counts (sample):")
targets_to_check = [TARGET] + [c for c in panel.columns if c.startswith('target_pv_')]
for t in targets_to_check:
    if t in panel.columns:
        print(f"  {t}: {panel[t].isna().sum()} NaNs")

# Build modeling DataFrame for a single target (replace if you want multi-horizon loop)
selected_target = TARGET                # swap to 'target_pv_h_1h' etc. when training horizons
df_model = panel[features + [selected_target, 'timestamp']].copy()

# Drop rows where the selected target is NaN (these are the shifted rows at dataset end)
before = len(df_model)
df_model = df_model.dropna(subset=[selected_target])
after = len(df_model)
print(f"\nDropped {before-after} rows with NaN in the target '{selected_target}'.")

# Also drop rows with NaN in any feature (alternatively you can impute)
before = len(df_model)
df_model = df_model.dropna(subset=features)
after = len(df_model)
print(f"Dropped {before-after} additional rows with NaN in features (you can change this to impute instead).")

from sklearn.model_selection import train_test_split

# Define variables for splitting
USE_TIME_SPLIT = False # Set to True for time-based split, False for random split
TEST_FRACTION = 0.2 # Fraction of data to use for testing

# Prepare X and y
X = df_model[features]
y = df_model[selected_target]

# Drop rows with NaN in the target variable before splitting
y = y.dropna()
X = X.loc[y.index] # Keep only the corresponding rows in X

# Recommended: time-based split to avoid leakage
if USE_TIME_SPLIT:
    # sort by time and split by index (keeps continuity)
    df_model = df_model.sort_values('timestamp').reset_index(drop=True)
    split_idx = int((1 - TEST_FRACTION) * len(df_model))
    X_train = df_model.loc[:split_idx-1, features]
    X_test  = df_model.loc[split_idx:, features]
    y_train = df_model.loc[:split_idx-1, selected_target]
    y_test  = df_model.loc[split_idx:, selected_target]
    print(f"Time-based split -> Train rows: {len(X_train)}, Test rows: {len(X_test)}")
else:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_FRACTION, random_state=42)
    print(f"Random split -> Train rows: {len(X_train)}, Test rows: {len(X_test)}")

"""# Traning Baseline and Initial ML models  - Experiment 1"""

# @title
from sklearn.dummy import DummyRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# --- Baseline model: persistence ---
baseline = DummyRegressor(strategy="mean")
baseline.fit(X_train, y_train)

# --- Random Forest ---
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# --- XGBoost ---
xgbr = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    objective='reg:squarederror',
    n_jobs=-1,
    random_state=42
)
xgbr.fit(X_train, y_train)

"""Model Evaluation"""

# @title
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def evaluate_model(model, X_test, y_test, name="Model"):
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}")
    return y_pred

y_pred_baseline = evaluate_model(baseline, X_test, y_test, "Baseline")
y_pred_rf = evaluate_model(rf, X_test, y_test, "Random Forest")
y_pred_xgb = evaluate_model(xgbr, X_test, y_test, "XGBoost")

# @title
import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
plt.plot(y_test.values[:200], label="Actual", color='blue', linewidth=2)
plt.plot(y_pred_rf[:200], label="Random Forest", color='orange', linestyle='--', marker='x', alpha=0.8)
plt.plot(y_pred_xgb[:200], label="XGBoost", color='green', linestyle='-', marker='s', alpha=0.6)
plt.title("PV Power Prediction (Sample 200 Points)")
plt.xlabel("Time Step (5-minute intervals)")
plt.ylabel("Power Output (kW)")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import xgboost as xgb

# Compute feature importances from trained XGBoost model
xgb_feat_imp = pd.Series(xgbr.feature_importances_, index=X.columns).sort_values(ascending=False)

# Plot top 15 features
plt.figure(figsize=(10,6))
sns.barplot(x=xgb_feat_imp[:15], y=xgb_feat_imp.index[:15], palette="coolwarm")
plt.title("Top 15 Feature Importances (XGBoost Model)", fontsize=14, weight='bold')
plt.xlabel("Feature Importance Score", fontsize=12)
plt.ylabel("Feature Name", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create feature importance series
feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Plot top 15 features
plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp[:15], y=feat_imp.index[:15], palette="viridis")
plt.title("Top 15 Feature Importances (Random Forest)", fontsize=14, weight='bold')
plt.xlabel("Feature Importance Score", fontsize=12)
plt.ylabel("Feature Name", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

errors = y_test.values - y_pred_xgb
plt.figure(figsize=(8,5))
sns.histplot(errors, bins=40, kde=True, color='skyblue')
plt.title("Distribution of Prediction Errors (XGBoost)")
plt.xlabel("Prediction Error (kW)")
plt.ylabel("Frequency")
plt.show()

"""Hyperparameter Tuning"""

# @title
from sklearn.model_selection import RandomizedSearchCV
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# -------------------------
# 1. Take a subset of training data for fast tuning
SAMPLE_FRAC = 0.1  # 10% of the training data
X_train_sub = X_train.sample(frac=SAMPLE_FRAC, random_state=42)
y_train_sub = y_train.loc[X_train_sub.index]

# -------------------------
# 2. Random Forest tuning (fast)
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf_random = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),
    param_distributions=rf_param_grid,
    n_iter=5,              # only 5 random combos
    cv=2,                  # 2-fold CV for speed
    scoring='neg_root_mean_squared_error',
    verbose=2,
    random_state=42
)
rf_random.fit(X_train_sub, y_train_sub)

print("Best RF params:", rf_random.best_params_)

# -------------------------
# 3. XGBoost tuning (fast)
xgb_param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_random = RandomizedSearchCV(
    estimator=xgb.XGBRegressor(
        objective='reg:squarederror',
        n_jobs=-1,
        random_state=42
    ),
    param_distributions=xgb_param_grid,
    n_iter=8,              # only 8 random combos
    cv=2,
    scoring='neg_root_mean_squared_error',
    verbose=2,
    random_state=42
)
xgb_random.fit(X_train_sub, y_train_sub)

print("Best XGB params:", xgb_random.best_params_)

"""# Train Baseline and Initial ML Models - Experiment 2

raning Baseline, RF & XGBoost models and saving
"""

# ------------------------------
# Train initial models
# ------------------------------
from sklearn.dummy import DummyRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import joblib  # for saving models

# --- Baseline model: persistence ---
baseline = DummyRegressor(strategy="mean")
baseline.fit(X_train, y_train)
joblib.dump(baseline, "baseline_model.pkl")  # save baseline

# --- Random Forest ---
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
joblib.dump(rf, "rf_model.pkl")  # save RF

# --- XGBoost ---
xgbr = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    objective='reg:squarederror',
    n_jobs=-1,
    random_state=42
)
xgbr.fit(X_train, y_train)
joblib.dump(xgbr, "xgb_model.pkl")  # save XGB

print("All initial models trained and saved!")

"""Model Evaluation"""

# ------------------------------
# Evaluate trained models
# ------------------------------
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def evaluate_model(model, X, y, name="Model"):
    y_pred = model.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    mae = mean_absolute_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    print(f"{name} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}")
    return y_pred

# Evaluate initial models
y_pred_baseline = evaluate_model(baseline, X_test, y_test, "Baseline")
y_pred_rf = evaluate_model(rf, X_test, y_test, "Random Forest")
y_pred_xgb = evaluate_model(xgbr, X_test, y_test, "XGBoost")

# Plot sample predictions (first 200 points)
plt.figure(figsize=(12,6))
plt.plot(y_test.values[:200], label="Actual", marker='o')
plt.plot(y_pred_rf[:200], label="Random Forest", marker='x')
plt.plot(y_pred_xgb[:200], label="XGBoost", marker='s')
plt.title("PV Power Prediction (Sample 200 points)")
plt.xlabel("Time step")
plt.ylabel("Power Output (kW)")
plt.legend()
plt.show()

# Feature importance for RF
feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp[:15], y=feat_imp.index[:15])
plt.title("Top 15 Feature Importances (Random Forest)")
plt.show()

"""Hyperparameter Tuning"""

# ------------------------------
# Hyperparameter tuning using a subset of data
# ------------------------------
from sklearn.model_selection import RandomizedSearchCV

# Use 10% of training data for fast tuning
SAMPLE_FRAC = 0.1
X_train_sub = X_train.sample(frac=SAMPLE_FRAC, random_state=42)
y_train_sub = y_train.loc[X_train_sub.index]

# --- Random Forest tuning ---
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf_random = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),
    param_distributions=rf_param_grid,
    n_iter=5,  # 5 random combinations
    cv=2,      # 2-fold CV
    scoring='neg_root_mean_squared_error',
    verbose=2,
    random_state=42
)
rf_random.fit(X_train_sub, y_train_sub)
print("Best RF params:", rf_random.best_params_)

# --- XGBoost tuning ---
xgb_param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_random = RandomizedSearchCV(
    estimator=xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42),
    param_distributions=xgb_param_grid,
    n_iter=8,
    cv=2,
    scoring='neg_root_mean_squared_error',
    verbose=2,
    random_state=42
)
xgb_random.fit(X_train_sub, y_train_sub)
print("Best XGB params:", xgb_random.best_params_)

"""Train Final Models with Tuned Parameters and Save"""

# ------------------------------
# Train final models with tuned parameters
# ------------------------------

# Random Forest with tuned params
rf_final = RandomForestRegressor(
    n_estimators=rf_random.best_params_['n_estimators'],
    max_depth=rf_random.best_params_['max_depth'],
    min_samples_split=rf_random.best_params_['min_samples_split'],
    min_samples_leaf=rf_random.best_params_['min_samples_leaf'],
    random_state=42,
    n_jobs=-1
)
rf_final.fit(X_train, y_train)
joblib.dump(rf_final, "rf_final_model.pkl")
print("Random Forest final model saved.")

# XGBoost with tuned params
xgb_final = xgb.XGBRegressor(
    n_estimators=xgb_random.best_params_['n_estimators'],
    max_depth=xgb_random.best_params_['max_depth'],
    learning_rate=xgb_random.best_params_['learning_rate'],
    subsample=xgb_random.best_params_['subsample'],
    colsample_bytree=xgb_random.best_params_['colsample_bytree'],
    objective='reg:squarederror',
    n_jobs=-1,
    random_state=42
)
xgb_final.fit(X_train, y_train)
joblib.dump(xgb_final, "xgb_final_model.pkl")
print("XGBoost final model saved.")

"""Evaluate Final Tuned Models"""

# ------------------------------
# Evaluate final tuned models
# ------------------------------
evaluate_model(rf_final, X_test, y_test, "Random Forest Final")
evaluate_model(xgb_final, X_test, y_test, "XGBoost Final")

# ----------------- Event-based classification metrics for regression models -----------------
import pandas as pd
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report

# Paths (edit if different)
PRED_DIR = "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed/predictions_ml"
# collect preds (change patterns as your files)
pred_files = {
    'rf': f"{PRED_DIR}/pred_rf_quick.parquet",
    'xgb': f"{PRED_DIR}/pred_xgb_quick.parquet",
    # 'lstm': "/content/drive/.../lstm_fixed_test_preds.parquet"   # add if available
}

# Load one of them to get structure
dfs = {}
for name, fp in pred_files.items():
    try:
        dfs[name] = pd.read_parquet(fp)
    except Exception as e:
        print("Warning: could not load", fp, e)

# choose a file that contains y_true (ground truth target) and station_id + timestamp
# unify into a single DataFrame keyed by (station_id, timestamp)
# prefer RF/XGB quick outputs created earlier where y_true is in the preds file
if not dfs:
    raise RuntimeError("No prediction files found in PRED_DIR. Put pred_*.parquet there.")

# merge on station_id + timestamp
merged = None
for name, dfp in dfs.items():
    # rename predictions column to include model name
    predcols = [c for c in dfp.columns if c.startswith('pred_') or c in ['pred_rf','pred_xgb','pred_lstm','pred']]
    if not predcols:
        # heuristics: look for anything not station/timestamp/y_true/column names
        for c in dfp.columns:
            if c not in ('station_id','timestamp','y_true','y_true_kW','y_pred'):
                predcols.append(c)
    predcol = predcols[0]
    dfp = dfp.rename(columns={predcol: f"pred_{name}"})
    # ensure timestamp dtype
    if 'timestamp' in dfp.columns:
        dfp['timestamp'] = pd.to_datetime(dfp['timestamp'])
    if merged is None:
        merged = dfp.copy()
    else:
        merged = merged.merge(dfp[['station_id','timestamp', f"pred_{name}"]], on=['station_id','timestamp'], how='outer')

# now determine ground-truth target - many of your preds files include 'y_true'
if 'y_true' in merged.columns:
    merged['y_true_kw'] = merged['y_true']
else:
    # try to load panel/test ground truth from combined set
    # user should ensure a test set with ground truth is available
    raise RuntimeError("No 'y_true' column in prediction files. Please include y_true in saved preds or load test set y.")

# need capacity to define normalized thresholds
# try to load capacity from panel_features_subset
FEATURES_FILE = "/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed/panel_features_subset.parquet"
feat = pd.read_parquet(FEATURES_FILE)[['station_id','capacity_kw']].drop_duplicates(subset=['station_id'])
merged = merged.merge(feat, on='station_id', how='left')

# define event: low generation at forecast horizon (e.g., target < 10% of capacity)
THRESH_PCT = 0.10
merged['event_true'] = (merged['y_true_kw'] < (THRESH_PCT * merged['capacity_kw'])).astype(int)

# compute event predictions per model
results = []
for name in dfs.keys():
    pred_col = f"pred_{name}"
    if pred_col not in merged.columns:
        continue
    # treat predicted kw value: predict event if < THRESH_PCT * capacity
    merged['event_pred'] = (merged[pred_col] < (THRESH_PCT * merged['capacity_kw'])).astype(int)
    pr, rc, f1, _ = precision_recall_fscore_support(merged['event_true'], merged['event_pred'], average='binary', zero_division=0)
    cm = confusion_matrix(merged['event_true'], merged['event_pred'])
    results.append({'model': name, 'precision': pr, 'recall': rc, 'f1': f1, 'n': len(merged)})

res_df = pd.DataFrame(results).sort_values('f1', ascending=False)
print("Event-based classification metrics (threshold {:.0%} of capacity):".format(THRESH_PCT))
print(res_df)

# >>> SHAP analysis for XGBoost and RandomForest (summary + dependence + comparison)
# Run in Colab after you have X_train (or feature DataFrame) in memory and models exported.
# Install shap if needed: !pip install shap --quiet

import os
import joblib
import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# ---------- CONFIG ----------
MODEL_DIR = Path("/content")            # <<-- adjust to folder where your models are stored
OUT_DIR = MODEL_DIR / "shap_outputs"
OUT_DIR.mkdir(exist_ok=True, parents=True)

# file paths - edit if your filenames differ
XGB_PKL = MODEL_DIR / "xgb_final_model.pkl"    # or xgb_quickjob.pkl
RF_PKL  = MODEL_DIR / "rf_final_model.pkl"     # or rf_quickjob.pkl

SAMPLE_FRAC = 0.05        # fraction of X_train to use for SHAP (reduce memory/time)
RANDOM_STATE = 42
MAX_DISPLAY = 20          # number of top features to show in summary

# ---------- CHECK & LOAD FEATURE DATA ----------
# You must have X_train (pandas DataFrame of features) prepared in your notebook.
# If X_train is not in memory, load a saved features parquet that matches the features used in training:
if 'X_train' not in globals():
    # try to find a candidate features parquet in processed/
    cand = [
        Path("/content/processed/panel_features_subset.parquet"),
        Path("/content/processed/panel_with_lags_short.parquet"),
        Path("/content/processed/panel_features.parquet"),
        Path("/content/processed/combined_panel_subset.parquet"),
        Path("/content/processed/combined_panel.parquet"),
        Path("/content/panel_features_subset.parquet"),
    ]
    X_train = None
    for p in cand:
        if p.exists():
            print("Loading features from:", p)
            df_all = pd.read_parquet(p)
            # reconstruct features as in training: drop target/metadata columns (adjust below if needed)
            exclude = {'timestamp','station_id','power_output_kw','has_optimizer','capacity_kw'}
            exclude |= set([c for c in df_all.columns if c.startswith('target_')])
            features = [c for c in df_all.columns if c not in exclude]
            print("Detected features count:", len(features))
            X_train = df_all[features].dropna().sample(frac=0.1, random_state=RANDOM_STATE)  # small sample to prevent huge memory
            break
    if X_train is None:
        raise FileNotFoundError("X_train not found in memory and no candidate features parquet found. Load your training features first.")
else:
    # X_train exists in memory; sample it for SHAP
    print("Using X_train from notebook memory.")
    # ensure it's a DataFrame
    if not isinstance(X_train, pd.DataFrame):
        X_train = pd.DataFrame(X_train)
    # drop NA rows for sampled features
    X_train = X_train.dropna()
    # take a small sample to keep SHAP fast
    X_train = X_train.sample(frac=min(SAMPLE_FRAC, 1.0), random_state=RANDOM_STATE)

print("SHAP sample shape:", X_train.shape)
X_sample = X_train.copy()

# ---------- HELPER to load model robustly ----------
def safe_joblib_load(path):
    if not Path(path).exists():
        print("Model file not found:", path)
        return None
    try:
        m = joblib.load(path)
        print("Loaded model:", path.name, " type:", type(m))
        return m
    except Exception as e:
        print("Joblib load error for", path, ":", e)
        return None

xgb_model = safe_joblib_load(XGB_PKL)
rf_model  = safe_joblib_load(RF_PKL)

# ---------- COMPUTE SHAP for each model ----------
results = {}
for name, model in [('XGB', xgb_model), ('RF', rf_model)]:
    if model is None:
        print(f"Skipping {name}: model not available.")
        continue

    print(f"\n=== Computing SHAP for {name} ===")
    try:
        explainer = shap.TreeExplainer(model)          # TreeExplainer works for both tree-based models
        # newer shap versions: use explainer(X) which returns an Explanation object
        shap_exp = explainer(X_sample)                # returns shap.Explanation
        shap_vals = shap_exp.values                    # shape (n_samples, n_features)
        base_values = shap_exp.base_values
        # store
        results[name] = {'explainer': explainer, 'shap_values': shap_vals, 'base_values': base_values}
        # Summary plot (matplotlib)
        plt.figure(figsize=(8,6))
        shap.summary_plot(shap_vals, X_sample, show=True, max_display=MAX_DISPLAY)
        plt.suptitle(f"SHAP summary: {name}", fontsize=12)
        plt.tight_layout()
        plt.savefig(OUT_DIR / f"shap_summary_{name}.png", dpi=200, bbox_inches='tight')
        print("Saved summary plot:", OUT_DIR / f"shap_summary_{name}.png")
        # Dependence plot for top feature
        mean_abs = np.abs(shap_vals).mean(axis=0)
        top_idx = np.argmax(mean_abs)
        top_feature = X_sample.columns[top_idx]
        print(f"Top feature for {name}:", top_feature)
        plt.figure(figsize=(6,5))
        shap.dependence_plot(top_feature, shap_vals, X_sample, show=True)
        plt.suptitle(f"SHAP dependence ({name}): {top_feature}", fontsize=12)
        plt.tight_layout()
        plt.savefig(OUT_DIR / f"shap_dependence_{name}_{top_feature}.png", dpi=200, bbox_inches='tight')
        print("Saved dependence plot:", OUT_DIR / f"shap_dependence_{name}_{top_feature}.png")
        # save arrays (optional)
        np.save(OUT_DIR / f"shap_values_{name}.npy", shap_vals)
        X_sample.reset_index(drop=True).to_parquet(OUT_DIR / f"shap_X_sample_{name}.parquet", index=False)
        print("Saved SHAP arrays and X_sample for", name)
    except Exception as e:
        print("Error computing SHAP for", name, ":", e)

# ---------- Compare importances between models ----------
print("\n=== Comparing mean |SHAP| importances across models ===")
imp_list = []
for name, d in results.items():
    shap_vals = d['shap_values']
    mean_abs = np.abs(shap_vals).mean(axis=0)
    df_imp = pd.DataFrame({'feature': X_sample.columns, f'{name}_mean_abs_shap': mean_abs})
    imp_list.append(df_imp)

if len(imp_list) >= 1:
    compare_df = imp_list[0]
    for df_imp in imp_list[1:]:
        compare_df = compare_df.merge(df_imp, on='feature', how='outer')
    # fill NA with 0
    compare_df = compare_df.fillna(0)
    # compute average rank and sort
    cols = [c for c in compare_df.columns if c != 'feature']
    compare_df['avg_importance'] = compare_df[cols].mean(axis=1)
    compare_df = compare_df.sort_values('avg_importance', ascending=False).reset_index(drop=True)
    print(compare_df.head(15))
    # save compare table
    compare_df.to_csv(OUT_DIR / "shap_feature_comparison.csv", index=False)
    print("Saved comparison CSV:", OUT_DIR / "shap_feature_comparison.csv")

    # quick barplot for top 10 features showing both models side-by-side (if both exist)
    topk = compare_df['feature'].head(10).tolist()
    fig, ax = plt.subplots(figsize=(8,4))
    idx = np.arange(len(topk))
    width = 0.35
    x_vals = np.arange(len(topk))
    # plot for each model present
    offsets = 0
    colors = ['#1f77b4', '#ff7f0e']
    for i, name in enumerate(results.keys()):
        vals = compare_df.set_index('feature')[f'{name}_mean_abs_shap'].loc[topk].values
        ax.bar(x_vals + offsets, vals, width=0.35, label=name, color=colors[i%len(colors)])
        offsets += width
    ax.set_xticks(x_vals + width/2)
    ax.set_xticklabels(topk, rotation=45, ha='right')
    ax.set_ylabel("Mean |SHAP|")
    ax.set_title("Top features: mean |SHAP| comparison")
    ax.legend()
    plt.tight_layout()
    plt.savefig(OUT_DIR / "shap_top_features_comparison.png", dpi=200, bbox_inches='tight')
    print("Saved comparison plot:", OUT_DIR / "shap_top_features_comparison.png")

print("\nDone. SHAP outputs saved in:", OUT_DIR)

"""Explainability with SHAP

SHAP TreeExplainer was applied to XGBoost and Random Forest models to quantify each feature’s contribution. SHAP summary plots display global importance and directionality; dependence plots show interactions and non-linear effects of individual features.
"""

!pip install shap lime --quiet

# SHAP explanation for tree models (XGBoost / RF)
import shap, joblib, numpy as np, pandas as pd
from pathlib import Path



# load model and feature data (use subset or sample)
xgb_model = joblib.load("/content/xgb_final_model.pkl")   # adjust name if different
rf_model  = joblib.load("/content/rf_final_model.pkl")

# use X_train (from your previous context). If X_train is huge, sample:
sample_frac = 0.05
X_sample = X_train.sample(frac=sample_frac, random_state=42)
print("SHAP sample shape:", X_sample.shape)

# TreeExplainer for XGBoost (fast)
explainer_xgb = shap.TreeExplainer(xgb_model)
shap_values_xgb = explainer_xgb.shap_values(X_sample)    # array shape (n_samples, n_features)

# Plot SHAP summary
shap.summary_plot(shap_values_xgb, X_sample, max_display=20, show=True)

# Dependence plot for a top feature (replace 'Irradiance' with your top feature)
top_feature = X_sample.columns[np.argmax(np.abs(shap_values_xgb).mean(axis=0))]
print("Top feature:", top_feature)
shap.dependence_plot(top_feature, shap_values_xgb, X_sample, show=True)

# For Random Forest (TreeExplainer also works)
explainer_rf = shap.TreeExplainer(rf_model)
shap_values_rf = explainer_rf.shap_values(X_sample)
shap.summary_plot(shap_values_rf, X_sample, max_display=20, show=True)

# Save computed SHAP values (optional)
np.save(MODEL_DIR / "shap_values_xgb.npy", shap_values_xgb)
X_sample.to_parquet(MODEL_DIR / "shap_X_sample.parquet", index=False)
print("SHAP values and sample saved.")

"""# Multi Horizon Forecasting

Predict photovoltaic (PV) power output at multiple future horizons (1-hour, 4-hour, 24-hour ahead) using past PV generation and meteorological data.                                                                           For each station, use historical PV generation, lags, rolling statistics, and weather features as predictors.

Define targets at multiple horizons: target_pv_h_1h, target_pv_h_4h, target_pv_h_24h.

Train models to predict each horizon separately
"""

import warnings
warnings.filterwarnings("ignore")
import pandas as pd, numpy as np, joblib, os
from pathlib import Path
from sklearn.dummy import DummyRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
PROCESSED = BASE / "processed"
FEATURES_FILE = PROCESSED / "panel_features_subset.parquet"
MODEL_DIR = PROCESSED / "models_multihorizon"
PRED_DIR = PROCESSED / "predictions_multihorizon"
MODEL_DIR.mkdir(parents=True, exist_ok=True)
PRED_DIR.mkdir(parents=True, exist_ok=True)

"""Define forecast horizons and model parameters"""

HORIZONS = {
    'h_1h': 'target_pv_h_1h',
    'h_4h': 'target_pv_h_4h',
    'h_24h': 'target_pv_h_24h'
}

RF_PARAMS = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42, 'n_jobs': -1}
XGB_PARAMS = {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1,
              'objective':'reg:squarederror', 'n_jobs': -1, 'random_state': 42}
TEST_FRAC = 0.2

"""Load the preprocessed dataset"""

panel = pd.read_parquet(FEATURES_FILE)
panel['timestamp'] = pd.to_datetime(panel['timestamp'])
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)

"""Automatically detect usable feature columns"""

exclude_cols = ['timestamp', 'station_id']
targets = list(HORIZONS.values())
exclude_all = exclude_cols + targets
features = [c for c in panel.columns if c not in exclude_all and c not in ['power_output_kw']]

"""Define custom evaluation metric (MAPE)"""

def mape(y_true, y_pred):
    denom = np.where(np.abs(y_true) < 1e-6, 1e-6, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100

"""Loop through each forecast horizon"""

for hkey, target_col in HORIZONS.items():
    ...

"""Clean data and create time-based train/test split"""

df = panel[features + [target_col, 'timestamp', 'station_id']].copy()
df = df.dropna(subset=[target_col])
df = df.dropna(subset=features)
df = df.sort_values('timestamp').reset_index(drop=True)
split_idx = int((1 - TEST_FRAC) * len(df))
train_df = df.iloc[:split_idx]
test_df = df.iloc[split_idx:]

"""Train three models: Baseline, Random Forest, XGBoost"""

baseline = DummyRegressor(strategy="mean")
baseline.fit(X_train, y_train)

rf = RandomForestRegressor(**RF_PARAMS)
rf.fit(X_train, y_train)

xgbr = xgb.XGBRegressor(**XGB_PARAMS)
xgbr.fit(X_train, y_train)

"""Model Evaluation"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def evaluate_model(model, X_test, y_test, name="Model"):
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}")
    return y_pred

y_pred_baseline = evaluate_model(baseline, X_test, y_test, "Baseline")
y_pred_rf = evaluate_model(rf, X_test, y_test, "Random Forest")
y_pred_xgb = evaluate_model(xgbr, X_test, y_test, "XGBoost")

"""Diagnostics"""

# Quick checks: ensure no targets are in features and check train/test sizes
print("Features intersecting targets (should be empty):", set(features).intersection({'target_pv_h_1h','target_pv_h_4h','target_pv_h_24h','power_output_kw'}))

# Show train/test lengths (if you still have X_train/X_test)
print("X_train, X_test:", X_train.shape, X_test.shape)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# predictions already computed? if not, run:
yhat_rf = rf.predict(X_test)
yhat_xgb = xgbr.predict(X_test)

# residuals
res_rf = y_test - yhat_rf
res_xgb = y_test - yhat_xgb

# scatter actual vs predicted (sample)
n = min(1000, len(y_test))
plt.figure(figsize=(6,6))
plt.scatter(y_test[:n], yhat_xgb[:n], s=8, alpha=0.4)
plt.plot([y_test[:n].min(), y_test[:n].max()], [y_test[:n].min(), y_test[:n].max()], 'r--')
plt.xlabel('Actual'); plt.ylabel('Predicted (XGBoost)')
plt.title('Actual vs Predicted (sample)')
plt.show()

# residual distribution
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.hist(res_rf, bins=60); plt.title('Residuals RF')
plt.subplot(1,2,2)
plt.hist(res_xgb, bins=60); plt.title('Residuals XGB')
plt.show()

# Print RMSE/MAE/R2 (repeat)
def metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return rmse, mae, r2
print("RF:", metrics(y_test, yhat_rf))
print("XGB:", metrics(y_test, yhat_xgb))

import pandas as pd
# Make a DataFrame with predictions and true values alongside station_id and timestamp
test_df = X_test.copy()
# if X_test is a slice without 'station_id' and 'timestamp', reload test_df from your earlier split
# For simplicity, assume you saved test_df earlier; otherwise recreate time-split and keep those columns.
# Example if test_df exists as in the multihorizon cell:
test_df = test_df.reset_index(drop=True)  # ensure aligned
pred_df = pd.DataFrame({
    'station_id': test_df['station_id'] if 'station_id' in test_df.columns else panel.loc[X_test.index,'station_id'].values,
    'y_true': y_test,
    'pred_rf': yhat_rf,
    'pred_xgb': yhat_xgb
})
# group by station
per_station = pred_df.groupby('station_id').apply(lambda g: pd.Series({
    'n': len(g),
    'rmse_rf': np.sqrt(((g['y_true']-g['pred_rf'])**2).mean()),
    'mae_rf': (g['y_true']-g['pred_rf']).abs().mean(),
    'rmse_xgb': np.sqrt(((g['y_true']-g['pred_xgb'])**2).mean()),
    'mae_xgb': (g['y_true']-g['pred_xgb']).abs().mean()
})).reset_index()
per_station = per_station.sort_values('rmse_xgb', ascending=False)
display(per_station.head(10))  # worst stations by XGB RMSE

# pick worst station and plot actual vs predicted over time
worst_station = per_station.iloc[0]['station_id']
print("Worst station:", worst_station)

# reconstruct test rows for that station (you need test_df with timestamp and station_id)
s_rows = pred_df[pred_df['station_id']==worst_station].copy()
# if timestamps exist:
# s_rows['timestamp'] = panel.loc[X_test.index,'timestamp']  # adjust as needed

plt.figure(figsize=(12,4))
plt.plot(s_rows['y_true'].values, label='Actual')
plt.plot(s_rows['pred_xgb'].values, label='XGBoost')
plt.plot(s_rows['pred_rf'].values, label='RandomForest')
plt.title(f'Station {worst_station} - test set predictions')
plt.legend()
plt.show()

"""merged panel dataframe - panel"""

# Assuming your merged panel DataFrame is called `panel`
# and it has 'station_id' and 'timestamp' columns

# Get train/test indices
train_idx = X_train.index
test_idx = X_test.index

# Build sets of (station, timestamp)
train_keys = set(zip(panel.loc[train_idx, 'station_id'], panel.loc[train_idx, 'timestamp'].astype(str)))
test_keys = set(zip(panel.loc[test_idx, 'station_id'], panel.loc[test_idx, 'timestamp'].astype(str)))

# Check overlap
overlap = train_keys.intersection(test_keys)
print("Overlapping (station, timestamp) count between train/test:", len(overlap))

"""In the diagnostic phase of the multi-horizon PV forecasting implementation, successfully validated the feature preparation, data alignment, and model performance across multiple stations. The train/test split produced datasets of 839,132 and 209,784 samples, respectively, and checks confirmed no major feature leakage apart from initially overlapping target columns. Model evaluation showed excellent predictive accuracy, with Random Forest achieving an RMSE of 384.06 kW (R² = 0.999) and XGBoost further improving to an RMSE of 231.68 kW (R² ≈ 1.000), significantly outperforming the baseline persistence model. Station-level diagnostics revealed strong overall model generalization, though high-capacity stations like Shaw Auditorium_Inverter displayed higher error magnitudes. Additionally, an overlap count of 139,445 station–timestamp pairs between train and test sets highlighted potential data leakage risks, prompting a shift toward a strictly temporal split for the next stage of multi-horizon forecasting.

# **DL MODEL TRANING**
"""

# Cell 1: Setup & load
import os
import numpy as np
import pandas as pd
from pathlib import Path
import joblib
import tensorflow as tf

# Paths - edit if needed
BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
PROCESSED = BASE / "processed"
FEATURES_FILE = PROCESSED / "panel_features_subset.parquet"
MODEL_DIR = PROCESSED / "dl_models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# Config
TARGET = "target_pv_h_1h"   # 1-hour ahead target
SEQ_LEN = 12                # input sequence length (12*5min = 1 hour)
BATCH_SIZE = 128
EPOCHS = 60
VALIDATION_SPLIT = 0.1      # fraction of training sequences for validation
RANDOM_SEED = 42
tf.random.set_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# Load dataset
panel = pd.read_parquet(FEATURES_FILE)
panel['timestamp'] = pd.to_datetime(panel['timestamp'])
panel = panel.sort_values(['station_id','timestamp']).reset_index(drop=True)
print("Loaded panel shape:", panel.shape)
print("Columns sample:", panel.columns.tolist()[:20])

"""Choose features, drop NaNs, scale"""

# Cell 2: select features and scale
from sklearn.preprocessing import StandardScaler

# Auto-detect features (exclude timestamp, station_id, targets)
exclude = {'timestamp','station_id'}
targets = [c for c in panel.columns if c.startswith('target_pv_')]
exclude.update(targets)
exclude.add('power_output_kw')  # don't include raw power as a direct future leak (optional)
features = [c for c in panel.columns if c not in exclude]

print("Using features (example):", features[:10], " ... total:", len(features))

# Drop rows with NaN in the target or features (or you can impute)
df = panel[features + [TARGET, 'station_id', 'timestamp']].copy()
before = len(df)
df = df.dropna(subset=[TARGET])
df = df.dropna(subset=features)
print(f"Dropped {before - len(df)} rows with NaN in target/features. Remaining rows: {len(df)}")

# Fit scaler on a sample (faster) or full training set later
scaler = StandardScaler()
# sample a subset to fit scaler to avoid memory issues, but ensure it's representative
sample_for_scaler = df.sample(frac=0.2, random_state=RANDOM_SEED)[features]
scaler.fit(sample_for_scaler)
joblib.dump(scaler, MODEL_DIR / "scaler_seq_1h.joblib")
print("Scaler fitted and saved.")
# transform full feature columns
df[features] = scaler.transform(df[features])

"""Train/validation/test split (time-based)"""

# Cell 4: time-based split by timestamp (global)
# Use the 'df' DataFrame created in the previous cell
# Assuming df contains 'station_id', 'timestamp', and the scaled features and target
meta_df = df[['station_id','timestamp']].copy() # Extract metadata for sorting

# sort by timestamp and station_id to maintain sequence integrity within stations
df = df.sort_values(['station_id','timestamp']).reset_index(drop=True)

# Create sequences
def create_sequences(df, seq_len, target_col, features):
    X, y, meta = [], [], []
    # Group by station to create sequences within each station
    for station_id, group in df.groupby('station_id'):
        data = group[features].values
        target = group[target_col].values
        # Ensure we have enough data for at least one sequence
        if len(group) >= seq_len:
            for i in range(len(group) - seq_len):
                X.append(data[i:(i + seq_len)])
                y.append(target[i + seq_len]) # Target is the value *after* the sequence
                meta.append((station_id, group['timestamp'].iloc[i + seq_len])) # Timestamp of the target

    return np.array(X), np.array(y), meta

# Create sequences from the cleaned and scaled DataFrame
X_all, y_all, meta_all = create_sequences(df, SEQ_LEN, TARGET, features)

# Sort sequences by timestamp globally for time-based split
meta_df_seq = pd.DataFrame(meta_all, columns=['station_id','timestamp'])
order = np.argsort(meta_df_seq['timestamp'].values)

X_all = X_all[order]
y_all = y_all[order]
meta_df_seq = meta_df_seq.iloc[order].reset_index(drop=True)


n_total = len(y_all)
split_train = int(0.8 * n_total)
split_val = int(0.9 * n_total)  # last 10% is test
X_train_seq, X_val_seq, X_test_seq = X_all[:split_train], X_all[split_train:split_val], X_all[split_val:]
y_train_seq, y_val_seq, y_test_seq = y_all[:split_train], y_all[split_train:split_val], y_all[split_val:]

print("Train sequences:", X_train_seq.shape, "Val:", X_val_seq.shape, "Test:", X_test_seq.shape)

"""# LSTM model"""

# Cell 1: setup & ensure capacity estimate
import numpy as np, pandas as pd, joblib
from pathlib import Path

BASE = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy")
PROCESSED = BASE / "processed"
FEATURES_FILE = PROCESSED / "panel_features_subset.parquet"   # edit if different
MODEL_DIR = PROCESSED / "dl_models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

panel = pd.read_parquet(FEATURES_FILE)
panel['timestamp'] = pd.to_datetime(panel['timestamp'])
panel = panel.sort_values(['station_id', 'timestamp']).reset_index(drop=True)
print("Loaded panel:", panel.shape)

# Ensure capacity_kw exists (if not, estimate 99th percentile per station)
if 'capacity_kw' not in panel.columns:
    print("Estimating capacity_kw per station from 99th percentile of observed power...")
    cap_est = panel.groupby('station_id')['power_output_kw'].quantile(0.99).reset_index().rename(columns={'power_output_kw':'capacity_kw'})
    cap_est['capacity_kw'] = cap_est['capacity_kw'].clip(lower=0.1)  # avoid zeros
    panel = panel.merge(cap_est, on='station_id', how='left')
else:
    print("Found existing capacity_kw column.")

# Quick check
print(panel[['station_id','capacity_kw']].drop_duplicates().shape, "stations")
panel['pv_norm'] = panel['power_output_kw'] / (panel['capacity_kw'] + 1e-9)  # normalized target (0..~1)
print("pv_norm sample:", panel['pv_norm'].describe().to_dict())

"""Select features, drop NaNs, fit feature scaler"""

# Cell 2: select features and fit StandardScaler for input features
from sklearn.preprocessing import StandardScaler
import numpy as np

TARGET = 'target_pv_h_1h'   # 1-hour ahead (shifted target must already exist in panel)
SEQ_LEN = 12                # 12*5min = 1 hour history
RANDOM_SEED = 42

# Build features list (exclude timestamps, station_id and target columns)
exclude = {'timestamp','station_id'}
targets = [c for c in panel.columns if c.startswith('target_pv_')]
exclude.update(targets)
features = [c for c in panel.columns if c not in exclude and c != 'power_output_kw' and c != 'pv_norm']
# include meteorological + engineered features (not raw power or target)
print("Using features (count):", len(features), "example:", features[:10])

# keep only rows with non-null target and feature values
df = panel[features + [TARGET, 'pv_norm', 'station_id', 'timestamp', 'capacity_kw']].copy()
before = len(df)
df = df.dropna(subset=[TARGET] + features)
print(f"Dropped {before-len(df)} rows with NaNs; remaining {len(df)} rows")

# Fit StandardScaler on features (sample for speed)
scaler = StandardScaler()
sample_frac = 0.2
scaler.fit(df[features].sample(frac=sample_frac, random_state=RANDOM_SEED))
joblib.dump(scaler, MODEL_DIR / "scaler_features.joblib")
print("Feature scaler saved.")
# transform features
df[features] = scaler.transform(df[features])

"""Create sequences and carry capacity for inverse-scaling"""

# Cell 3: create sequences per station, include capacity for inverse transform
import numpy as np

def create_sequences_with_capacity(df, features, target_col, seq_len=12):
    X_list, y_list, capacity_list, meta = [], [], [], []
    # group by station to avoid leakage
    for st, grp in df.groupby('station_id'):
        grp = grp.sort_values('timestamp')
        feat_arr = grp[features].values
        targ_arr = grp[target_col].values   # this is absolute kW target
        cap = grp['capacity_kw'].values
        n = len(grp)
        # window sliding
        for i in range(0, n - seq_len):
            x = feat_arr[i:i+seq_len]
            y = targ_arr[i+seq_len]      # absolute kW target at horizon
            c = cap[i+seq_len]           # capacity at target timestamp
            if np.isnan(y) or np.isnan(x).any():
                continue
            # normalize target by capacity -> pv_norm_target (0..1)
            y_norm = y / (c + 1e-9)
            X_list.append(x.astype('float32'))
            y_list.append(y_norm.astype('float32'))
            capacity_list.append(c.astype('float32'))
            meta.append((st, grp['timestamp'].iloc[i+seq_len]))
    X = np.array(X_list)
    y = np.array(y_list)
    capacity_arr = np.array(capacity_list)
    return X, y, capacity_arr, meta

# Create sequences
X_all, y_all_norm, capacity_arr, meta = create_sequences_with_capacity(df, features, TARGET, seq_len=SEQ_LEN)
print("Sequences created:", X_all.shape, y_all_norm.shape, capacity_arr.shape)

"""Train / val / test split (time-based global split)"""

# Cell 4: time-based split (global order by meta timestamp)
import numpy as np
import pandas as pd

meta_df = pd.DataFrame(meta, columns=['station_id','timestamp'])
meta_df['timestamp'] = pd.to_datetime(meta_df['timestamp'])

order = np.argsort(meta_df['timestamp'].values)
X_all = X_all[order]; y_all_norm = y_all_norm[order]; capacity_arr = capacity_arr[order]; meta_df = meta_df.iloc[order].reset_index(drop=True)

n = len(y_all_norm)
train_end = int(0.8 * n)
val_end = int(0.9 * n)

X_train_seq, X_val_seq, X_test_seq = X_all[:train_end], X_all[train_end:val_end], X_all[val_end:]
y_train_norm, y_val_norm, y_test_norm = y_all_norm[:train_end], y_all_norm[train_end:val_end], y_all_norm[val_end:]
cap_train, cap_val, cap_test = capacity_arr[:train_end], capacity_arr[train_end:val_end], capacity_arr[val_end:]

print("Train/Val/Test shapes:", X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)

"""Build, compile and train LSTM (with lower LR)"""

# Cell 5: train LSTM on normalized target, lower LR and better callbacks
import tensorflow as tf
from tensorflow.keras import models, layers, callbacks
from tensorflow.keras.optimizers import Adam
import math

tf.keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

SEQ_LEN = X_train_seq.shape[1]
N_FEATS = X_train_seq.shape[2]
LSTM_UNITS = 64
BATCH_SIZE = 64            # smaller batch may help convergence
EPOCHS = 80

model_lstm = models.Sequential([
    layers.Input(shape=(SEQ_LEN, N_FEATS)),
    layers.LSTM(LSTM_UNITS, return_sequences=False),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='linear')   # predicts normalized target (pv fraction)
])

opt = Adam(learning_rate=1e-4)   # lower LR recommended
model_lstm.compile(optimizer=opt, loss='mse', metrics=['mae'])
model_lstm.summary()

# Callbacks - save as native Keras (.keras)
ckp_path = MODEL_DIR / "lstm_1h_best.keras"
ckp = callbacks.ModelCheckpoint(
    filepath=str(ckp_path),
    save_best_only=True,
    monitor='val_loss',
    mode='min'
)
es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = model_lstm.fit(
    X_train_seq, y_train_norm,
    validation_data=(X_val_seq, y_val_norm),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[ckp, es],s
    verbose=2
)

print("Training finished. Best model saved to:", ckp_path)

"""Load best model, predict, invert normalization to kW, evaluate in kW"""

# Debug & fix broadcasting, then compute metrics and a sample plot
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Add prediction step
y_pred_norm = model_lstm.predict(X_test_seq)

# Check shapes
print("y_pred_norm shape:", np.asarray(y_pred_norm).shape)
print("y_test_norm shape:", np.asarray(y_test_norm).shape)
print("cap_test shape (raw):", np.asarray(cap_test).shape)

# Make sure y_pred_norm and y_test_norm are 1D
y_pred_norm = np.asarray(y_pred_norm).squeeze()
y_test_norm = np.asarray(y_test_norm).squeeze()
print("After squeeze - y_pred_norm shape:", y_pred_norm.shape, "y_test_norm shape:", y_test_norm.shape)

# Fix cap_test to 1D:
cap_test_arr = np.asarray(cap_test)
if cap_test_arr.ndim == 2:
    # if second dim is 1, squeeze; if it's >1 take first column (safety)
    if cap_test_arr.shape[1] == 1:
        cap_test_flat = cap_test_arr.squeeze()
    else:
        # If accidentally 2 columns identical or one is extraneous, choose first column
        cap_test_flat = cap_test_arr[:,0]
        print("Warning: cap_test had >1 column; using first column as capacity.")
else:
    cap_test_flat = cap_test_arr

# Final shape check
print("cap_test_flat shape:", cap_test_flat.shape)

# Ensure lengths match
n_pred = len(y_pred_norm)
n_cap = len(cap_test_flat)
n_true = len(y_test_norm)
if not (n_pred == n_cap == n_true):
    raise ValueError(f"Length mismatch: y_pred={n_pred}, cap={n_cap}, y_true={n_true}")

# Convert normalized predictions back to kW
y_pred_kw = y_pred_norm * cap_test_flat
y_true_kw = y_test_norm * cap_test_flat

# Compute metrics
rmse = np.sqrt(mean_squared_error(y_true_kw, y_pred_kw))
mae = mean_absolute_error(y_true_kw, y_pred_kw)
r2 = r2_score(y_true_kw, y_pred_kw)
print(f"LSTM (1h) -> RMSE: {rmse:.3f} kW, MAE: {mae:.3f} kW, R²: {r2:.4f}")

# Normalized metric (MAE as fraction of capacity) - useful for cross-station comparison
norm_mae = np.mean(np.abs((y_true_kw - y_pred_kw) / (cap_test_flat + 1e-9))) * 100
print(f"Normalized MAE (as % of capacity): {norm_mae:.3f}%")

# Plot a sample slice
n_plot = min(300, len(y_true_kw))
plt.figure(figsize=(12,4))
plt.plot(y_true_kw[:n_plot], label='Actual (kW)')
plt.plot(y_pred_kw[:n_plot], label='LSTM Pred (kW)')
plt.legend(); plt.title('LSTM: Actual vs Pred (sample)'); plt.show()

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import pandas as pd

# --- 1) Ensure y_test_norm is 1-D ---
y_test_norm_arr = np.asarray(y_test_norm)
if y_test_norm_arr.ndim == 2:
    if y_test_norm_arr.shape[1] == 1:
        y_test_norm_flat = y_test_norm_arr.squeeze()
        print("y_test_norm had shape (n,1); squeezed to 1-D.")
    else:
        # If there are 2 columns, warn and use first column (likely duplication)
        print(f"WARNING: y_test_norm has shape {y_test_norm_arr.shape}. Using first column as ground-truth normalized target.")
        y_test_norm_flat = y_test_norm_arr[:,0]
else:
    y_test_norm_flat = y_test_norm_arr

# Ensure y_pred_norm 1-D
y_pred_norm = np.asarray(y_pred_norm).squeeze()

# cap_test_flat should already be 1-D from previous step; re-check
cap_test_arr = np.asarray(cap_test)
if cap_test_arr.ndim == 2:
    if cap_test_arr.shape[1] == 1:
        cap_test_flat = cap_test_arr.squeeze()
    else:
        # we already used first column earlier, but reassign explicitly to be safe
        cap_test_flat = cap_test_arr[:,0]
        print("WARNING: cap_test had >1 column; using first column as capacity (again).")
else:
    cap_test_flat = cap_test_arr

# Final sanity checks
n_pred = len(y_pred_norm)
n_true = len(y_test_norm_flat)
n_cap  = len(cap_test_flat)
print(f"Lengths -> y_pred: {n_pred}, y_test: {n_true}, cap: {n_cap}")
if not (n_pred == n_true == n_cap):
    raise ValueError("Length mismatch after fixes: cannot proceed. Check sequence creation.")

# --- 2) Convert normalized preds back to kW and compute metrics ---
y_pred_kw = y_pred_norm * cap_test_flat
y_true_kw = y_test_norm_flat * cap_test_flat

rmse = np.sqrt(mean_squared_error(y_true_kw, y_pred_kw))
mae = mean_absolute_error(y_true_kw, y_pred_kw)
r2 = r2_score(y_true_kw, y_pred_kw)
norm_mae_pct = np.mean(np.abs((y_true_kw - y_pred_kw) / (cap_test_flat + 1e-9))) * 100

print(f"\nLSTM (1h) metrics:")
print(f"  RMSE: {rmse:.3f} kW")
print(f"  MAE : {mae:.3f} kW")
print(f"  R²  : {r2:.4f}")
print(f"  Normalized MAE: {norm_mae_pct:.3f}% (mean absolute error as % of station capacity)")

# --- 3) Plot sample of predictions vs actual ---
n_plot = min(300, len(y_true_kw))
plt.figure(figsize=(12,4))
plt.plot(y_true_kw[:n_plot], label='Actual (kW)', linewidth=1)
plt.plot(y_pred_kw[:n_plot], label='LSTM Pred (kW)', linewidth=1)
plt.legend(); plt.title('LSTM: Actual vs Pred (sample)'); plt.show()

# --- 4) Per-station normalized MAE diagnostic (top worst stations) ---
# We need station ids for each test sample; use meta_df (it was created earlier) to get station_id for test slice
try:
    # meta_df aligns with the full sequence array ordering; test slice is meta_df.iloc[val_end:]
    # If meta_df and indices differ, adapt accordingly.
    station_ids_test = meta_df['station_id'].iloc[val_end:].values  # val_end defined in split cell
    # Build DataFrame
    df_test = pd.DataFrame({
        'station_id': station_ids_test,
        'y_true_kw': y_true_kw,
        'y_pred_kw': y_pred_kw,
        'capacity_kw': cap_test_flat
    })
    df_test['abs_err'] = (df_test['y_true_kw'] - df_test['y_pred_kw']).abs()
    df_test['norm_abs_err_pct'] = df_test['abs_err'] / (df_test['capacity_kw'] + 1e-9) * 100

    per_station = df_test.groupby('station_id').agg(
        n=('y_true_kw','size'),
        rmse_kw=(lambda x: np.sqrt(np.mean((df_test.loc[x.index,'y_true_kw'] - df_test.loc[x.index,'y_pred_kw'])**2))), # placeholder will be replaced
    )
    # simpler per-station normalized MAE:
    per_station = df_test.groupby('station_id').agg(
        n=('y_true_kw','size'),
        mae_kw=('abs_err','mean'),
        norm_mae_pct=('norm_abs_err_pct','mean')
    ).reset_index().sort_values('norm_mae_pct', ascending=False)

    print("\nTop 10 worst stations by normalized MAE (% of capacity):")
    display(per_station.head(10))
except Exception as e:
    print("Could not compute per-station diagnostics automatically:", e)
    print("If meta_df/val_end are not in memory, ensure you load or recreate them from the sequence creation step.")

# --- 5) Short diagnostic note for you to inspect ---
print("\nDiagnostic note:")
print("- y_test_norm had extra columns; this often happens if y was created as column vector (n,1) and later stacked or reshaped.")
print("- To avoid this in future: ensure when creating y arrays you append scalars, and use np.array(y_list).squeeze() immediately.")

import numpy as np, pandas as pd

# Inspect shapes again
print("Shapes: y_pred_norm", np.asarray(y_pred_norm).shape,
      "y_test_norm", np.asarray(y_test_norm).shape,
      "cap_test", np.asarray(cap_test).shape)

# Examine a small sample of y_test_norm and cap_test
ytn = np.asarray(y_test_norm)
ct = np.asarray(cap_test)

print("\nFirst 10 rows of y_test_norm (raw):")
print(ytn[:10])

print("\nFirst 10 rows of cap_test (raw):")
print(ct[:10])

# If cap_test has >1 column, show both columns' stats (min/max/mean)
if ct.ndim == 2:
    for col in range(ct.shape[1]):
        col_vals = ct[:,col].astype(float)
        print(f"\ncap_test column {col} stats: min={col_vals.min():.6f}, mean={col_vals.mean():.6f}, max={col_vals.max():.6f}")
else:
    col_vals = ct
    print(f"\ncap_test stats: min={col_vals.min():.6f}, mean={col_vals.mean():.6f}, max={col_vals.max():.6f}")

# Check y_test_norm columns
if ytn.ndim == 2:
    for col in range(ytn.shape[1]):
        colv = ytn[:,col].astype(float)
        print(f"\ny_test_norm column {col} stats: min={colv.min():.6f}, mean={colv.mean():.6f}, max={colv.max():.6f}")
else:
    print("\ny_test_norm is 1D; stats:", np.min(ytn), np.mean(ytn), np.max(ytn))

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pandas as pd
import matplotlib.pyplot as plt

# --- flatten arrays safely ---
y_pred_norm = np.asarray(y_pred_norm).squeeze()
ytn = np.asarray(y_test_norm)
ct = np.asarray(cap_test)

# Fix y_test_norm -> pick first column only if 2D (but check if columns differ)
if ytn.ndim == 2:
    # check equality of columns
    if np.allclose(ytn[:,0], ytn[:,1]):
        print("y_test_norm columns are identical; using first column.")
    else:
        print("WARNING: y_test_norm columns differ; using first column but inspect earlier code.")
    y_test_flat = ytn[:,0]
else:
    y_test_flat = ytn

# Fix cap_test -> pick sensible column if 2D
if ct.ndim == 2:
    # show correlation to y_test_flat to pick the best column
    corr0 = np.corrcoef(ct[:,0], y_test_flat)[0,1]
    corr1 = np.corrcoef(ct[:,1], y_test_flat)[0,1]
    print(f"Correlation y_test vs cap col0: {corr0:.4f}, col1: {corr1:.4f}")
    # prefer the column with reasonable nonzero mean
    means = [np.mean(ct[:,0]), np.mean(ct[:,1])]
    print("Capacity column means:", means)
    # pick first column if it's sensible
    if means[0] > 1e-6:
        cap_flat = ct[:,0]
    elif means[1] > 1e-6:
        cap_flat = ct[:,1]
    else:
        # fallback: use max across columns
        cap_flat = np.max(ct, axis=1)
        print("Both capacity cols tiny; using row-wise max as fallback.")
else:
    cap_flat = ct

# sanity lengths
assert len(y_pred_norm) == len(y_test_flat) == len(cap_flat), "Length mismatch after flattening."

# recover kW scale
y_pred_kw = y_pred_norm * cap_flat
y_true_kw = y_test_flat * cap_flat

# compute metrics
rmse = np.sqrt(mean_squared_error(y_true_kw, y_pred_kw))
mae = mean_absolute_error(y_true_kw, y_pred_kw)
r2 = r2_score(y_true_kw, y_pred_kw)
norm_mae_pct = np.mean(np.abs((y_true_kw - y_pred_kw) / (cap_flat + 1e-9))) * 100

print("\nCorrected LSTM (1h) metrics:")
print(f"  RMSE: {rmse:.3f} kW")
print(f"  MAE : {mae:.3f} kW")
print(f"  R²  : {r2:.4f}")
print(f"  Normalized MAE: {norm_mae_pct:.3f}%")

# quick plot
n_plot = min(300, len(y_true_kw))
plt.figure(figsize=(12,4))
plt.plot(y_true_kw[:n_plot], label='Actual (kW)', lw=1)
plt.plot(y_pred_kw[:n_plot], label='Pred (kW)', lw=1)
plt.legend(); plt.title('LSTM Actual vs Pred (sample)'); plt.show()

# per-station normalized MAE (if meta_df and indices align)
try:
    # meta_df is the full sequence meta; val_end should be defined where train/val/test split happened
    # If meta_df exists: test station list is meta_df.iloc[val_end:]['station_id']
    station_ids_test = meta_df['station_id'].iloc[val_end:].values
    df_test = pd.DataFrame({
        'station_id': station_ids_test,
        'y_true_kw': y_true_kw,
        'y_pred_kw': y_pred_kw,
        'capacity': cap_flat
    })
    df_test['abs_err'] = (df_test['y_true_kw'] - df_test['y_pred_kw']).abs()
    df_test['norm_abs_err_pct'] = df_test['abs_err'] / (df_test['capacity'] + 1e-9) * 100
    per_station = df_test.groupby('station_id').agg(
        n=('y_true_kw','size'),
        mae_kw=('abs_err','mean'),
        norm_mae_pct=('norm_abs_err_pct','mean')
    ).reset_index().sort_values('norm_mae_pct', ascending=False)
    print("\nTop 10 worst stations by normalized MAE (%):")
    display(per_station.head(10))
except Exception as e:
    print("Could not compute per-station table automatically:", e)
    print("If meta_df/val_end are not in memory, re-create them or provide station_ids_test array.")

# --- FIX NORMALIZATION & COMPUTE CORRECT METRICS ---
import numpy as np, pandas as pd
from pathlib import Path
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# --- CONFIG: adjust if your names differ ---
# panel: DataFrame with original feature-engineered data (should contain 'station_id' and maybe 'capacity_kw' or 'power_output_kw')
# meta_df: DataFrame of sequence metadata created earlier with columns ['station_id','timestamp'] (aligned with all sequences)
# val_end: integer index split boundary used earlier (train_end, val_end etc.)
# y_pred_norm: model output array (may be normalized or already in kW)
# y_test_norm: test-target array (may be normalized or already in kW)
# cap_test: capacity array used earlier (may have been normalized incorrectly)

# Check required variables exist
missing = [name for name in ['panel','meta_df','val_end','y_pred_norm','y_test_norm'] if name not in globals()]
if missing:
    raise RuntimeError(f"Missing variables in workspace needed for fix: {missing}.\n"
                       "Please load panel, recreate meta_df and sequence split, or run earlier cells that created them.")

# 1) Reconstruct true capacity per station in kW
panel_df = panel.copy()
if 'capacity_kw' in panel_df.columns and panel_df['capacity_kw'].notna().sum()>0:
    capacity_table = panel_df.groupby('station_id')['capacity_kw'].first().reset_index()
    print("Using existing capacity_kw from panel.")
else:
    # Estimate capacity as 99th percentile of observed power_output_kw per station
    print("Estimating capacity_kw from 99th percentile of observed power_output_kw per station (fallback).")
    if 'power_output_kw' not in panel_df.columns:
        raise RuntimeError("No 'capacity_kw' or 'power_output_kw' in panel DataFrame to estimate true capacity.")
    capacity_table = panel_df.groupby('station_id')['power_output_kw'].quantile(0.99).reset_index().rename(columns={'power_output_kw':'capacity_kw'})
    # clip tiny values
    capacity_table['capacity_kw'] = capacity_table['capacity_kw'].clip(lower=0.1)

# Map capacity by station_id
capacity_map = dict(zip(capacity_table['station_id'], capacity_table['capacity_kw']))

# 2) Get station ids for the test sample using meta_df and val_end
if not isinstance(meta_df, pd.DataFrame):
    raise RuntimeError("meta_df must be a pandas DataFrame created during sequence generation.")

# test sample station list is meta_df.iloc[val_end:]
if val_end is None:
    raise RuntimeError("val_end not found. It should be the index where the validation/test split begins.")
station_ids_test = meta_df['station_id'].iloc[val_end:].values
n_test = len(station_ids_test)
print(f"Recovered {n_test} test samples from meta_df starting at index {val_end}.")

# Build true capacity array (kW) per test sample
true_capacities_kw = np.array([ capacity_map.get(s, np.nan) for s in station_ids_test ], dtype=float)
if np.isnan(true_capacities_kw).any():
    nnan = np.isnan(true_capacities_kw).sum()
    print(f"Warning: {nnan} test samples had missing capacity mapping — filling with station-wise median capacity.")
    # fill missing with median of capacity_table
    true_capacities_kw = np.where(np.isnan(true_capacities_kw), np.nanmedian(capacity_table['capacity_kw']), true_capacities_kw)

print("True capacity (kW) stats: min={:.3f}, mean={:.3f}, max={:.3f}".format(
    np.min(true_capacities_kw), np.mean(true_capacities_kw), np.max(true_capacities_kw)
))

# 3) Inspect y_pred_norm and y_test_norm ranges to detect units
yp = np.asarray(y_pred_norm).squeeze()
yt = np.asarray(y_test_norm)

print("\nDetecting units/ranges:")
print("y_pred_norm: shape", yp.shape, "min/max/mean:", np.nanmin(yp), np.nanmax(yp), np.nanmean(yp))
print("y_test_norm raw: shape", yt.shape, "min/max/mean:", np.nanmin(yt), np.nanmax(yt), np.nanmean(yt))

# Flatten y_test if 2-D (take first column after confirming similarity)
if yt.ndim == 2:
    if yt.shape[1] == 1 or np.allclose(yt[:,0], yt[:,1]):
        yt_flat = yt[:,0]
        print("y_test_norm had 2 columns identical or single-col; using first column.")
    else:
        # If columns differ, choose the column that looks like kW (large values) or matches yp scale
        # Heuristics: if one column has mean > 10 and the other small, choose the large one
        means = [np.nanmean(yt[:,i]) for i in range(yt.shape[1])]
        pick = int(np.argmax(np.abs(means)))
        yt_flat = yt[:,pick]
        print(f"y_test_norm had 2 differing columns; chose column {pick} based on absolute mean.")
else:
    yt_flat = yt

# Decide whether yp and yt_flat are already in kW or normalized (0..1)
def looks_like_kw(arr):
    # heuristics: if mean or max > 1000, treat as kW; else if max <= 2 treat as normalized fraction
    if np.nanmax(np.abs(arr)) > 1000:
        return True
    if np.nanmax(np.abs(arr)) <= 2:
        return False
    # fallback: compare to true capacity mean
    if np.nanmean(np.abs(arr)) > 0.5 * np.nanmean(true_capacities_kw):
        return True
    return False

yp_is_kw = looks_like_kw(yp)
yt_is_kw = looks_like_kw(yt_flat)

print(f"\nHeuristic detection: y_pred_norm is treated as {'kW' if yp_is_kw else 'normalized'}, y_test_norm is treated as {'kW' if yt_is_kw else 'normalized'}.")

# 4) Convert both to kW scale if needed
if yp_is_kw:
    y_pred_kw = yp
else:
    # yp is normalized fraction -> multiply by true capacities for each sample
    y_pred_kw = yp * true_capacities_kw

if yt_is_kw:
    y_true_kw = yt_flat
else:
    y_true_kw = yt_flat * true_capacities_kw

# Final sanity: lengths must match
if not (len(y_pred_kw) == len(y_true_kw) == len(true_capacities_kw)):
    raise RuntimeError(f"Length mismatch after conversion: pred={len(y_pred_kw)}, true={len(y_true_kw)}, cap={len(true_capacities_kw)}")

# 5) Compute metrics (kW) and normalized MAE%
rmse = np.sqrt(mean_squared_error(y_true_kw, y_pred_kw))
mae = mean_absolute_error(y_true_kw, y_pred_kw)
r2 = r2_score(y_true_kw, y_pred_kw)
norm_mae_pct = np.mean(np.abs((y_true_kw - y_pred_kw) / (true_capacities_kw + 1e-9))) * 100

print("\n--- Final corrected metrics ---")
print(f"RMSE: {rmse:.3f} kW")
print(f"MAE : {mae:.3f} kW")
print(f"R²  : {r2:.4f}")
print(f"Normalized MAE (mean % of capacity): {norm_mae_pct:.3f}%")

# 6) Quick per-station normalized MAE (top worst & best)
try:
    station_ids = station_ids_test  # array aligned to y arrays
    df_test = pd.DataFrame({
        'station_id': station_ids,
        'y_true_kw': y_true_kw,
        'y_pred_kw': y_pred_kw,
        'capacity_kw': true_capacities_kw
    })
    df_test['abs_err'] = (df_test['y_true_kw'] - df_test['y_pred_kw']).abs()
    df_test['norm_abs_err_pct'] = df_test['abs_err'] / (df_test['capacity_kw'] + 1e-9) * 100

    per_station = df_test.groupby('station_id').agg(
        n=('y_true_kw','size'),
        mae_kw=('abs_err','mean'),
        norm_mae_pct=('norm_abs_err_pct','mean')
    ).reset_index().sort_values('norm_mae_pct', ascending=False)

    print("\nTop 10 worst stations by normalized MAE%:")
    display(per_station.head(10))

    print("\nTop 10 best stations by normalized MAE%:")
    display(per_station.tail(10))
except Exception as e:
    print("Could not compute per-station summary automatically:", e)

# 7) Save corrected predictions and true values for later use (optional)
OUT_DIR = Path.cwd() / "fixed_results"
OUT_DIR.mkdir(exist_ok=True)
pd.DataFrame({'y_true_kw': y_true_kw, 'y_pred_kw': y_pred_kw, 'capacity_kw': true_capacities_kw, 'station_id': station_ids_test}).to_parquet(OUT_DIR / "lstm_fixed_test_preds.parquet", index=False)
print("\nSaved corrected test predictions to:", OUT_DIR / "lstm_fixed_test_preds.parquet")

weighted_norm_mae_pct = (
    np.sum(np.abs(y_true_kw - y_pred_kw)) /
    np.sum(true_capacities_kw)
) * 100
print(f"Weighted normalized MAE (capacity-weighted): {weighted_norm_mae_pct:.3f}%")

# Cell A: helpers & safety checks (run once)
import numpy as np, pandas as pd, math, os
from pathlib import Path
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import tensorflow as tf

# Model dir
MODEL_DIR = Path("/content/drive/MyDrive/Final Year Research Research -rooftop solar project feasibility  /Research Data/Dataset - original copy/processed/dl_models")
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# --- Robustly get test set capacity (kW) ---
# This requires access to the original 'panel' DataFrame, the test set indices,
# and preferably the station-wise capacity calculation from earlier.

# Check if 'panel' DataFrame exists and has necessary info
if 'panel' not in globals() or not isinstance(panel, pd.DataFrame) or 'station_id' not in panel.columns:
     raise RuntimeError("The original 'panel' DataFrame is required to get test set capacities (Cell A). Please ensure it's loaded.")

# Check if X_test is available (from train/test split)
if 'X_test' not in globals() or not isinstance(X_test, pd.DataFrame):
    # Fallback: Try to load the full test set from the sequence split if saved/accessible
    # This requires meta_df and val_end to be available
    if 'meta_df' in globals() and 'val_end' in globals():
        try:
            station_ids_test = meta_df['station_id'].iloc[val_end:].values
            # Recreate the capacity map from the panel DataFrame
            if 'capacity_map' not in globals() or not capacity_map:
                 print("Recreating station capacity map from panel DataFrame...")
                 if 'capacity_kw' in panel.columns and panel['capacity_kw'].notna().sum()>0:
                     capacity_table = panel.groupby('station_id')['capacity_kw'].first().reset_index()
                 elif 'power_output_kw' in panel.columns: # Fallback to 99th percentile
                     print("Estimating capacity_kw from 99th percentile of observed power_output_kw per station (fallback).")
                     capacity_table = panel.groupby('station_id')['power_output_kw'].quantile(0.99).reset_index().rename(columns={'power_output_kw':'capacity_kw'})
                 else:
                      raise RuntimeError("Cannot find capacity_kw or power_output_kw in panel DataFrame.")
                 capacity_table['capacity_kw'] = capacity_table['capacity_kw'].clip(lower=0.1) # Avoid zeros
                 capacity_map = dict(zip(capacity_table['station_id'], capacity_table['capacity_kw']))

            cap_test_final = np.array([ capacity_map.get(s, np.nan) for s in station_ids_test ], dtype=float)
            print("Built cap_test_final from panel/meta_df.")
        except Exception as e:
            raise RuntimeError("Cannot reconstruct test set capacity array from meta_df. Please ensure panel, meta_df, and val_end are available, or re-run sequence creation.") from e
    else:
         raise RuntimeError("X_test not found. Cannot get test set indices to retrieve capacities. Please run a train/test split cell first.")
else:
    # Preferred method: Get capacities using X_test indices from the original panel DataFrame
    print("Getting test set capacities using X_test indices from panel DataFrame.")
    if 'capacity_kw' in panel.columns and panel['capacity_kw'].notna().sum()>0:
        # Directly get capacity for the rows in X_test
        cap_test_final = panel.loc[X_test.index, 'capacity_kw'].values.astype(float)
    elif 'power_output_kw' in panel.columns: # Fallback to 99th percentile based on test station_ids
        print("Estimating capacity_kw from 99th percentile of observed power_output_kw for test stations (fallback).")
        test_station_ids = panel.loc[X_test.index, 'station_id'].unique()
        capacity_table = panel[panel['station_id'].isin(test_station_ids)].groupby('station_id')['power_output_kw'].quantile(0.99).reset_index().rename(columns={'power_output_kw':'capacity_kw'})
        capacity_table['capacity_kw'] = capacity_table['capacity_kw'].clip(lower=0.1)
        capacity_map = dict(zip(capacity_table['station_id'], capacity_table['capacity_kw']))
        cap_test_final = panel.loc[X_test.index, 'station_id'].map(capacity_map).values.astype(float)
    else:
        raise RuntimeError("Cannot find capacity_kw or power_output_kw in panel DataFrame to estimate test set capacity.")

# Handle potential NaNs in capacity_test_final (should be rare if panel had capacities)
if np.isnan(cap_test_final).any():
    nnan = np.isnan(cap_test_final).sum()
    print(f"Warning: {nnan} test samples had missing capacity mapping after reconstruction — filling with median capacity from all stations.")
    # Use median of all estimated capacities as a fallback
    if 'capacity_map' in globals():
         fallback_median_cap = np.nanmedian(list(capacity_map.values()))
    elif 'capacity_table' in globals():
         fallback_median_cap = np.nanmedian(capacity_table['capacity_kw'])
    else: # Last resort: median of the test capacity array itself
        fallback_median_cap = np.nanmedian(cap_test_final)

    cap_test_final = np.where(np.isnan(cap_test_final), fallback_median_cap, cap_test_final)
    print(f"Filled {nnan} NaNs in test capacities with median value: {fallback_median_cap:.3f}")


# Ensure cap_test_final is 1D
cap_test_final = np.asarray(cap_test_final).squeeze()
if cap_test_final.ndim > 1:
     raise RuntimeError(f"cap_test_final is not 1D after squeeze: shape {cap_test_final.shape}. Cannot proceed.")


print("cap_test_final stats (kW): min={:.3f}, mean={:.3f}, max={:.3f}".format(
    np.nanmin(cap_test_final), np.nanmean(cap_test_final), np.nanmax(cap_test_final)
))

# Prepare y_test_norm_flat (assuming it exists and is 1D or can be flattened)
if 'y_test_norm' in globals():
    ytn = np.asarray(y_test_norm)
    if ytn.ndim==2:
        # choose first column if identical otherwise choose column with larger magnitude
        if ytn.shape[1] == 1:
             y_test_norm_flat = ytn.squeeze()
             print("y_test_norm had shape (n,1); squeezed to 1-D.")
        elif np.allclose(ytn[:,0], ytn[:,1]):
            y_test_norm_flat = ytn[:,0]
            print("y_test_norm columns are identical; using first column.")
        else:
            # heuristic pick based on absolute mean
            pick = int(np.argmax(np.abs(np.nanmean(ytn,axis=0))))
            y_test_norm_flat = ytn[:,pick]
            print(f"y_test_norm had 2 differing columns; chose column {pick} based on absolute mean.")
    else:
        y_test_norm_flat = ytn
else:
    raise RuntimeError("y_test_norm not found. Create sequences and split before running this cell.")

# Confirm shapes match X_test_seq length if available
if 'X_test_seq' in globals():
    if len(y_test_norm_flat) != X_test_seq.shape[0]:
        print("Warning: y_test length != X_test_seq length. y_test:", len(y_test_norm_flat), "X_test:", X_test_seq.shape[0])
    if len(cap_test_final) != X_test_seq.shape[0]:
        print("Warning: cap_test_final length != X_test_seq length. cap_test_final:", len(cap_test_final), "X_test:", X_test_seq.shape[0])


print("y_test_norm_flat stats:", np.nanmin(y_test_norm_flat), np.nanmean(y_test_norm_flat), np.nanmax(y_test_norm_flat))

# Function to compute metrics (kW) once predictions (norm) are produced
def compute_metrics_from_norm(y_pred_norm, y_test_norm_flat, cap_test_final, name="Model"):
    """
    Computes evaluation metrics in kW scale given normalized predictions,
    flattened normalized true values, and final kW capacity array.
    """
    y_pred_norm = np.asarray(y_pred_norm).squeeze()
    y_test_norm_flat = np.asarray(y_test_norm_flat).squeeze()
    cap_test_final = np.asarray(cap_test_final).squeeze()

    if not (len(y_pred_norm) == len(y_test_norm_flat) == len(cap_test_final)):
         raise ValueError(f"Input length mismatch in compute_metrics_from_norm: pred={len(y_pred_norm)}, true={len(y_test_norm_flat)}, cap={len(cap_test_final)}")

    # detect if arrays already in kW (if max > 1000, treat as kW)
    yp_is_kw = np.nanmax(np.abs(y_pred_norm)) > 1000
    yt_is_kw = np.nanmax(np.abs(y_test_norm_flat)) > 1000

    if yp_is_kw:
        y_pred_kw = y_pred_norm
    else:
        y_pred_kw = y_pred_norm * cap_test_final

    if yt_is_kw:
        y_true_kw = y_test_norm_flat
    else:
         y_true_kw = y_test_norm_flat * cap_test_final


    # Handle potential inf/nan after scaling/inverting
    valid_indices = np.isfinite(y_true_kw) & np.isfinite(y_pred_kw) & (cap_test_final > 1e-9)
    if np.sum(valid_indices) < len(y_true_kw):
         print(f"Warning: Excluding {len(y_true_kw) - np.sum(valid_indices)} samples with inf/nan/zero capacity for metric calculation.")

    y_true_kw_valid = y_true_kw[valid_indices]
    y_pred_kw_valid = y_pred_kw[valid_indices]
    cap_test_valid = cap_test_final[valid_indices]


    rmse = np.sqrt(mean_squared_error(y_true_kw_valid, y_pred_kw_valid))
    mae = mean_absolute_error(y_true_kw_valid, y_pred_kw_valid)
    r2 = r2_score(y_true_kw_valid, y_pred_kw_valid)
    weighted_norm_mae_pct = (np.sum(np.abs(y_true_kw_valid - y_pred_kw_valid)) / (np.sum(cap_test_valid)+1e-9)) * 100

    return {'name':name, 'rmse':rmse, 'mae':mae, 'r2':r2, 'weighted_norm_mae_pct':weighted_norm_mae_pct,
            'y_true_kw':y_true_kw, 'y_pred_kw':y_pred_kw} # Return full arrays for plotting

print("Helpers ready.")